<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sentence Mining with OpenAI's Whisper | Alex McKinney</title><meta name=keywords content><meta name=description content="Online, I tend to market myself as an AI / computery guy, creating and following content related to these areas. However, I have other interests and passions to tinker with – unfortunately too many to actually dedicate lots of time to all of them, with the exception of one. The One is language learning, specifically learning Chinese, which I have been generally consistent with.
It has been a long road, starting in late 2018 but with large breaks and misguided routes taken along the way."><meta name=author content><link rel=canonical href=https://afmck.in/posts/2023-06-26-sentence-mining/><link crossorigin=anonymous href=/assets/css/stylesheet.min.0fbade1da18e97af7375719317bb9ec325a4f325a425cbb45b6b7c0f5ce82a5d.css integrity="sha256-D7reHaGOl69zdXGTF7uewyWk8yWkJcu0W2t8D1zoKl0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://afmck.in/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://afmck.in/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://afmck.in/favicon-32x32.png><link rel=apple-touch-icon href=https://afmck.in/apple-touch-icon.png><link rel=mask-icon href=https://afmck.in/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:title" content="Sentence Mining with OpenAI's Whisper"><meta property="og:description" content="Online, I tend to market myself as an AI / computery guy, creating and following content related to these areas. However, I have other interests and passions to tinker with – unfortunately too many to actually dedicate lots of time to all of them, with the exception of one. The One is language learning, specifically learning Chinese, which I have been generally consistent with.
It has been a long road, starting in late 2018 but with large breaks and misguided routes taken along the way."><meta property="og:type" content="article"><meta property="og:url" content="https://afmck.in/posts/2023-06-26-sentence-mining/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-04T07:07:39+01:00"><meta property="article:modified_time" content="2023-07-04T07:07:39+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Sentence Mining with OpenAI's Whisper"><meta name=twitter:description content="Online, I tend to market myself as an AI / computery guy, creating and following content related to these areas. However, I have other interests and passions to tinker with – unfortunately too many to actually dedicate lots of time to all of them, with the exception of one. The One is language learning, specifically learning Chinese, which I have been generally consistent with.
It has been a long road, starting in late 2018 but with large breaks and misguided routes taken along the way."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://afmck.in/posts/"},{"@type":"ListItem","position":3,"name":"Sentence Mining with OpenAI's Whisper","item":"https://afmck.in/posts/2023-06-26-sentence-mining/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Sentence Mining with OpenAI's Whisper","name":"Sentence Mining with OpenAI\u0027s Whisper","description":"Online, I tend to market myself as an AI / computery guy, creating and following content related to these areas. However, I have other interests and passions to tinker with – unfortunately too many to actually dedicate lots of time to all of them, with the exception of one. The One is language learning, specifically learning Chinese, which I have been generally consistent with.\nIt has been a long road, starting in late 2018 but with large breaks and misguided routes taken along the way.","keywords":[],"articleBody":"Online, I tend to market myself as an AI / computery guy, creating and following content related to these areas. However, I have other interests and passions to tinker with – unfortunately too many to actually dedicate lots of time to all of them, with the exception of one. The One is language learning, specifically learning Chinese, which I have been generally consistent with.\nIt has been a long road, starting in late 2018 but with large breaks and misguided routes taken along the way. Ultimately, in the past couple years I’ve been very consistent with vocabulary and reading practice. I’ve cracked the formula for learning in this regard, at least for me. By no means am I an expert, but on my recent trip to Taiwan I managed well with reading – better than expected. Progressing further is a matter of time rather than trying to hack my brain.\nRandom rice field in Taiwan.\nWith listening, however, I was worse than expected. There were a few of times I could fully understand a conversation. Most of the time I got the jist (smiling or laughing at tactical times) but not quick enough for a timely reply. Contrary to popular belief, reading Chinese is easier than listening.\nOf course, this is simply a function of time and effort put into practising listening. I have put in over a hundred times more work reading than listening, so naturally I am worse. Time spent on a skill over a long period of time mostly depends on how consistent you are at practising said skill. How consistent I am comes down to how frictionless it is to get started practising the skill.\nSome statistics from the pre-made deck I use to learn new vocabulary passively.\nAn example vocabulary card from the pre-made deck, which I use to acquire new vocabulary passively. I am just over 60% of the way to completing this deck.\nFor reading, I use apps such as Anki and get bits of practice in spare minutes here or there. It is easy to do this every day: usually while waiting in a queue, elevator, or public transport – times like these. Moreover, I have built a sufficient vocabulary base where I can read interesting content for fun, which also helps to stay consistent. Bar a handful of days, I have done Anki cuecards daily for the past year. Listening, however, is more labor intensive to get started. To improve, I need a method that makes it easy for lazy-ol’ me to be consistent.\nWays of practising listening In my time, I’ve explored many methods for reading and listening. A key part of language learning is finding a method that works well for you. Perhaps a bit of a cope, but when you see on Youtube “I learnt X fluently in Y weeks”, the same method may or may not work for you. Your goals may also be different: levels of fluency, whether you care about writing and reading, or just speaking and listening, types of content, and so on. It is highly personal and takes much trial and error.\nI’m happy to say for reading and vocabulary acquisition, I’ve found my method. For listening, I am still exploring. Some methods that I have and have not tried include:\nTextbooks with audio – These are good when getting started, but are not realistic examples and the content is often not very engaging. Podcasts – Good but the interesting content is too high-level whilst the simpler podcasts are less interesting, which equates to being harder to do every day. Immersion – This requires a higher level to do consistently, but would result in amazing gains once able. Shadowing – Listening to sentences and repeating them back to train ear and tongue in tandem. Very repetitive and boring. Courses – Courses like Pimsleur. Effective but expensive, and cannot tailor the content to stuff I care about. Tutoring – Great and relatively inexpensive with online services. However, I wouldn’t be able to do this every single day. Do in tandem with other methods. Literally talking to strangers – There are lots of Youtube polyglots who do a fair bit of this. To be honest the prospect terrifies me, but respect to them. The OG is Laoshu but there is a recent comeback with people like Ryan Hales to practice on apps like Omegle and OmeTv. Sentence Mining – Watching content with native subtitles and finding “$n+i$” sentences. These are sentences where you understand all but $i$ words (typically $i=1$) and then creating cuecards from them with audio. These are later reviewed using spaced repetition systems. Sentence mining is my current method. As it uses subtitles, you can use any reading capabilities to support listening. Furthermore, you can tailor it exactly to your interests by picking content you enjoy. For example, before going to Taiwan I watched videos by Taiwanese Youtubers to learn more about some interesting locations. Now, as I am about to move in with my girlfriend, I am watching cooking channels to learn the terminology, so we can cook together in her language. Out of all the methods I have tried, this is the method I like the most.\nThe first card I made through sentence mining, created manually. From the Taiwanese drama “Mom! Don’t do that!”\nI gave sentence mining a try half a year ago after seeing amazing progress using it for learning Japanese by my favourite Youtube channel Livakivi. He has been documenting his process of learning Japanese every day for over four years, creating 20 new cuecards every day using sentence mining, recently reaching 20,000 cards total. The results are fantastic, especially his fluency when speaking despite only practising speaking for a total of a few hours over four years.\nThis why I don’t focus on speaking, except random phrases for fun. It seems that building strong listening skills first helps to support speaking later.\nDespite sentence mining being a promising approach, it is also very labor-intensive. For each sentence you want to mine, you need to:\nWrite the full sentence in Anki. Highlight the target words. Write the readings (how to pronounce) of the target words. Write the definitions of the target words. Record the sentence audio using audio capture tools. Optionally, take a screenshot of the content. Storing the readings is important in Chinese as the characters alone don’t always indicate the pronunciation. It is also a tonal language, so I need to pay attention to tones in the words.\nThis is a lot. Without using tools you can easily spend more time creating cards than watching and concentrating on the content and the language. Livakivi, in his videos, uses Migaku to automate some of the process. Before that, he used an array of tools to make mining somewhat easier, but states that without Migaku he would have likely burnt out long before reaching 20,000 cards.\nSee his video for more details on his process for sentence mining Japanese content. There are lots of parallels between learning Japanese and Chinese, so I learnt a lot despite targeting different languages.\nI don’t have the god-like levels of discipline of Livakivi to create 20 cards a day manually, like I started out doing, and soon enough I stopped entirely. But as I mentioned, after returning from Taiwan I felt disappointed by my listening abilities and endeavoured to re-approach the problem.\nLike I said, to put in the time to get better I need to be consistent. In order to be consistent study needs to be frictionless. It can never be as easy as reviewing vocabulary cards, but I can try to make it as smooth as possible. And what better way than using ✨programming✨.\nThe Things I Did My goal was to write a program to make sentence mining from videos as easy as possible. I focused primarily on Youtube videos, but the same principles apply to local videos acquired through 🏴‍☠️legitimate means🏴‍☠️. The requirements were as follows:\nConverts to Anki so I can review on all devices, especially mobile. Also helps keep consistent by hooking into my existing Anki addiction (habit stacking). Reduce labour cost of creating cuecards so I can focus on the content I am mining and the language. Program needs to be portable. I travel frequently, so it should work on my Mac with a weak CPU, as well as on powerful desktops. Mobile would be cool, but not for now. Robust, nothing kills the mood more than tracebacks. Contradicting the last point a bit, I decided to make it an exercise in quickly creating a hacky solution, then gradually iterating on it. I wasn’t sure which ideas would work so I wanted to avoid prematurely overengineering a solution, only to hate using it.\nIteration 1 – Basic MVP The first iteration was basic. I began creating a script that takes a CSV file with the following fields:\nA sentence with Markdown bold tags highlighting target words. Definitions for each of the target words. Floating point values for the start and end times of the sentence in the content. Example CSV file. The first line is the target Youtube video URL, and all others are extracted sentences.\nThis script then generates readings all the target words using the Python library pinyin. Then, it uses youtube-dl to download the video and uses ffmpeg to extract audio and screenshots from the target regions. These are then formatted as another CSV file that is importable into Anki.\nAlthough basic, this is easier than using screen capture tools to manually create audio recordings and screenshots. I am still, however, bottlenecked by copying or writing the sentences and extracting precise, sub-second timestamps.\nIteration 2 – Enter Whisper Iteration 2 sought to iron out the task of creating the sentences and the timestamps. I stayed on brand and used ✨AI✨ to transcribe audio for me. Using youtube-dl to download the audio, I passed it to Whisper JAX and parsed the output into a CSV file in the same format as the one I created by hand in Iteration 1 – just with an empty definitions field.\nNow all I need to do is highlight the target words and write their definition; Whisper handles the transcription and timestamps for me. If a particular sentence is too easy or too hard, I can just delete the sentence line. Highlighting and deleting like this is a breeze using Vim.\nThere remain some issues with this approach. For one, I found the timestamps were not granular enough, usually just to the second. Secondly, sometimes these timesteps were simply inaccurate or had glitchy repeated sentences. Finally, although on GPU it ran blazing fast, it was a slow on my laptop CPU.\nIteration 3 – Whisper but slower The current iteration instead uses whisper.cpp – a zero dependency, optimised for CPU Whisper implementation, with limited GPU support to boot. This makes it more useable on laptop at the cost of slower desktop performance. In practice this doesn’t matter as I can simply do something else as the script runs. Furthermore, I’ve found the timestamps and transcriptions to be more accurate than the JAX implementation so far.\nExample transcript from whisper.cpp with timestamps on a short excerpt from JFK’s famous speech.\nThe cherry on top was installing a command line version of Google Translate trans. It is great for quickly checking the meaning of words by switching terminal focus, rather than using my phone dictionary. Google Translate is unreliable for long or nuanced sentences, but not bad for single words. If you, dear reader, know of a command line English-Chinese dictionary please let me know.\nLimitations The solution now is far better than manual work, but still not perfect. Remaining limitations include:\nTargets Youtube exclusively but could be extended to local videos. However, it cannot handle DRM-enabled content like on Netflix. Solution? 🏴‍️ 🤷 Some videos just don’t work with Whisper. For example Xiaoying Cuisine, a cooking channel, has many videos that Whisper just fails to transcribe, simply failing to print most the sentences. This happens with both implementations I tried, so it perhaps is an issue with the model itself. Cannot stream in videos, so longer content takes a while to process. Streaming is possible, but not necessary as I can start the script and do something else while it processes. I could also chunk up the video and transcribe each chunk at a time. Transcriptions are not 100% accurate, especially with certain accents. I wouldn’t recommend this method unless you are advanced enough to spot errors. This slows things down as I cannot fully trust Whisper, but overall it is very accurate and faster than manual transcription. Cannot use on mobile. This would be cool, but I know little about mobile development. The next step is to refactor the script now that I have a good proof of concept. It is currently an awful amalgamation of Python, binaries, and shell scripts. I would not add automatic definition generation. Chinese is a highly contextual, so often the meaning of words changes depending on context – on top of the nuance and ambiguity in all languages. As always, YMMV.\nConclusion Although I’ve defined a somewhat overkill method, I am quite lazy, so I know I need to proactively reduce friction in order to actually do things. This is somewhat contradictory, but it is what it is.\nAfter using it for the past week, it is definitely helping with my consistency. Not only is there less friction, it also feels good to use a tool you yourself made. Additionally, writing about this and my goals publicly provides some soft peer pressure which should also help remain consistent. We’ll see how that actually goes with time.\nIn general with computers, I feel it is nearly always worth the time to optimise your workflow in order to minimise the work required wherever possible. That is why I am so into using Linux for personal computing, but that is for another blog post entirely. This is just one instance of this ideology.\nLanguage learning itself is a topic I could talk at length about, but that is again for another time. I actually have a draft on this that I wrote before going to Taiwan, however it was so long that I couldn’t finish in time before leaving. This made a lot of the content totally out of date and hence is confined to a dead branch. Some day though.\nI had a few goals when writing this blog post. One, to show a cool use of AI for language learning. Two, to hopefully inspire people to identify and reduce friction in their own workflows. Finally, to practice writing short(er) form content.\nIf you liked this post please consider following me on Twitter or use this site’s RSS feed for notifications on future ramblings. Alternatively you can navigate to the root of this website and repeatedly refresh until something happens. Thank you for reading this far and I hope you found it useful!\nAcknowledgements and Extra Resources I take heavy inspiration from Livakivi’s Youtube Channel. I would recommend checking it out even if you aren’t interested in language learning. They cover many other topics like home renovation, drawing, technology, and just generally learning things. His full series on language learning can be found here.\nFound something wrong with this blog post? Let me know via email or Twitter!\n","wordCount":"2567","inLanguage":"en","datePublished":"2023-07-04T07:07:39+01:00","dateModified":"2023-07-04T07:07:39+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://afmck.in/posts/2023-06-26-sentence-mining/"},"publisher":{"@type":"Organization","name":"Alex McKinney","logo":{"@type":"ImageObject","url":"https://afmck.in/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://afmck.in accesskey=h title="Alex McKinney (Alt + H)">Alex McKinney</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://afmck.in/about title=About><span>About</span></a></li><li><a href=https://afmck.in/posts title=Posts><span>Posts</span></a></li><li><a href=https://afmck.in/cv.pdf title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Sentence Mining with OpenAI's Whisper</h1><div class=post-meta><span title='2023-07-04 07:07:39 +0100 +0100'>July 4, 2023</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#ways-of-practising-listening aria-label="Ways of practising listening">Ways of practising listening</a></li><li><a href=#the-things-i-did aria-label="The Things I Did">The Things I Did</a><ul><li><a href=#iteration-1--basic-mvp aria-label="Iteration 1 – Basic MVP">Iteration 1 – Basic MVP</a></li><li><a href=#iteration-2--enter-whisper aria-label="Iteration 2 – Enter Whisper">Iteration 2 – Enter Whisper</a></li><li><a href=#iteration-3--whisper-but-slower aria-label="Iteration 3 – Whisper but slower">Iteration 3 – Whisper but slower</a></li></ul></li><li><a href=#limitations aria-label=Limitations>Limitations</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a><ul><li><a href=#acknowledgements-and-extra-resources aria-label="Acknowledgements and Extra Resources">Acknowledgements and Extra Resources</a></li></ul></li></ul></div></details></div><div class=post-content><p>Online, I tend to market myself as an AI / computery guy, creating and following content related to these areas. However, I have other interests and passions to tinker with – unfortunately too many to actually dedicate lots of time to all of them, with the exception of one. The One is language learning, specifically learning Chinese, which I have been generally consistent with.</p><p>It has been a long road, starting in late 2018 but with large breaks and misguided routes taken along the way. Ultimately, in the past couple years I&rsquo;ve been very consistent with vocabulary and reading practice. I&rsquo;ve cracked the formula for learning in this regard, at least for me. By no means am I an expert, but on my recent trip to Taiwan I managed well with reading – better than expected. Progressing further is a matter of time rather than trying to hack my brain.</p><p><img loading=lazy src=taiwan.jpg alt="Rice field in Taiwan"></p><blockquote><p>Random rice field in Taiwan.</p></blockquote><p>With listening, however, I was worse than expected. There were a few of times I could fully understand a conversation. Most of the time I got the jist (smiling or laughing at tactical times) but not quick enough for a timely reply. Contrary to popular belief, reading Chinese is easier than listening.</p><p>Of course, this is simply a function of time and effort put into practising listening. I have put in over a hundred times more work reading than listening, so naturally I am worse. Time spent on a skill over a long period of time mostly depends on how consistent you are at practising said skill. How consistent I am comes down to how frictionless it is to get started practising the skill.</p><p><img loading=lazy src=image.png alt="Statistics from the Vocabulary Deck"></p><blockquote><p>Some statistics from the pre-made deck I use to learn new vocabulary passively.</p></blockquote><p><img loading=lazy src=image-1.png alt="Example vocabulary card form the pre-made deck."></p><blockquote><p>An example vocabulary card from the pre-made deck, which I use to acquire new vocabulary passively. I am just over 60% of the way to completing this deck.</p></blockquote><p>For reading, I use apps such as Anki and get bits of practice in spare minutes here or there. It is easy to do this every day: usually while waiting in a queue, elevator, or public transport – times like these. Moreover, I have built a sufficient vocabulary base where I can read interesting content for fun, which also helps to stay consistent. Bar a handful of days, I have done Anki cuecards daily for the past year. Listening, however, is more labor intensive to get started. To improve, I need a method that makes it easy for lazy-ol&rsquo; me to be consistent.</p><h2 id=ways-of-practising-listening>Ways of practising listening<a hidden class=anchor aria-hidden=true href=#ways-of-practising-listening>#</a></h2><p>In my time, I&rsquo;ve explored many methods for reading and listening. A key part of language learning is finding a method that works well for you. Perhaps a bit of a cope, but when you see on Youtube &ldquo;I learnt X fluently in Y weeks&rdquo;, the same method may or may not work for you. Your goals may also be different: levels of fluency, whether you care about writing and reading, or just speaking and listening, types of content, and so on. It is highly personal and takes much trial and error.</p><p>I&rsquo;m happy to say for reading and vocabulary acquisition, I&rsquo;ve found my method. For listening, I am still exploring. Some methods that I have and have not tried include:</p><ul><li><strong>Textbooks with audio</strong> – These are good when getting started, but are not realistic examples and the content is often not very engaging.</li><li><strong>Podcasts</strong> – Good but the interesting content is too high-level whilst the simpler podcasts are less interesting, which equates to being harder to do every day.</li><li><strong>Immersion</strong> – This requires a higher level to do consistently, but would result in amazing gains once able.</li><li><strong>Shadowing</strong> – Listening to sentences and repeating them back to train ear and tongue in tandem. Very repetitive and boring.</li><li><strong>Courses</strong> – Courses like Pimsleur. Effective but expensive, and cannot tailor the content to stuff I care about.</li><li><strong>Tutoring</strong> – Great and relatively inexpensive with online services. However, I wouldn&rsquo;t be able to do this every single day. Do in tandem with other methods.</li><li><strong>Literally talking to strangers</strong> – There are lots of Youtube polyglots who do a fair bit of this. To be honest the prospect terrifies me, but respect to them. The OG is <a href=https://www.youtube.com/@laoshu505000>Laoshu</a> but there is a recent comeback with people like <a href=https://www.youtube.com/@RyanHaleYT>Ryan Hales</a> to practice on apps like Omegle and OmeTv.</li><li><strong>Sentence Mining</strong> – Watching content with native subtitles and finding &ldquo;$n+i$&rdquo; sentences. These are sentences where you understand all but $i$ words (typically $i=1$) and then creating cuecards from them with audio. These are later reviewed using spaced repetition systems.</li></ul><p>Sentence mining is my current method. As it uses subtitles, you can use any reading capabilities to support listening. Furthermore, you can tailor it exactly to your interests by picking content you enjoy. For example, before going to Taiwan I watched videos by Taiwanese Youtubers to learn more about some interesting locations. Now, as I am about to move in with my girlfriend, I am watching cooking channels to learn the terminology, so we can cook together in her language. Out of all the methods I have tried, this is the method I like the most.</p><p><img loading=lazy src=image-2.png alt="My first card made manually through sentence mining. From &amp;ldquo;Mom! Don&amp;rsquo;t do that!&amp;rdquo;"></p><blockquote><p>The first card I made through sentence mining, created manually. From the Taiwanese drama &ldquo;Mom! Don&rsquo;t do that!&rdquo;</p></blockquote><p>I gave sentence mining a try half a year ago after seeing amazing progress using it for learning Japanese by my favourite Youtube channel <a href=https://www.youtube.com/c/livakivi>Livakivi</a>. He has been documenting his process of learning Japanese <strong>every day</strong> for over four years, creating 20 new cuecards every day using sentence mining, recently reaching 20,000 cards total. The results are fantastic, especially his fluency when speaking despite only practising speaking for a total of a few hours over four years.</p><blockquote><p>This why I don&rsquo;t focus on speaking, except random phrases for fun. It seems that building strong listening skills first helps to support speaking later.</p></blockquote><p>Despite sentence mining being a promising approach, it is also very labor-intensive. For each sentence you want to mine, you need to:</p><ul><li>Write the full sentence in Anki.</li><li>Highlight the target words.</li><li>Write the readings (how to pronounce) of the target words.</li><li>Write the definitions of the target words.</li><li>Record the sentence audio using audio capture tools.</li><li>Optionally, take a screenshot of the content.</li></ul><p><img loading=lazy src=image-3.png alt="To manually create cards, you would need to fill in this data manually in an interface like this."></p><blockquote><p>Storing the readings is important in Chinese as the characters alone don&rsquo;t always indicate the pronunciation. It is also a tonal language, so I need to pay attention to tones in the words.</p></blockquote><p>This is a lot. Without using tools you can easily spend more time creating cards than watching and concentrating on the content and the language. Livakivi, in his videos, uses <a href=https://www.migaku.io/>Migaku</a> to automate some of the process. Before that, he used an array of tools to make mining somewhat easier, but states that without Migaku he would have likely burnt out long before reaching 20,000 cards.</p><p><img loading=lazy src=image-4.png alt="Livakivi&amp;rsquo;s Playlist on learning Japanese"></p><blockquote><p>See <a href=https://youtu.be/QBcQJESGQvc>his video</a> for more details on his process for sentence mining Japanese content. There are lots of parallels between learning Japanese and Chinese, so I learnt a lot despite targeting different languages.</p></blockquote><p>I don&rsquo;t have the god-like levels of discipline of Livakivi to create 20 cards a day manually, like I started out doing, and soon enough I stopped entirely. But as I mentioned, after returning from Taiwan I felt disappointed by my listening abilities and endeavoured to re-approach the problem.</p><p>Like I said, to put in the time to get better I need to be consistent. In order to be consistent study needs to be frictionless. It can never be as easy as reviewing vocabulary cards, but I can try to make it as smooth as possible. And what better way than using ✨programming✨.</p><h2 id=the-things-i-did>The Things I Did<a hidden class=anchor aria-hidden=true href=#the-things-i-did>#</a></h2><p>My goal was to write a program to make sentence mining from videos as easy as possible. I focused primarily on Youtube videos, but the same principles apply to local videos acquired through 🏴‍☠️legitimate means🏴‍☠️. The requirements were as follows:</p><ul><li><strong>Converts to Anki</strong> so I can review on all devices, especially mobile. Also helps keep consistent by hooking into my existing Anki addiction (habit stacking).</li><li><strong>Reduce labour cost</strong> of creating cuecards so I can focus on the content I am mining and the language.</li><li>Program needs to be <strong>portable</strong>. I travel frequently, so it should work on my Mac with a weak CPU, as well as on powerful desktops. Mobile would be cool, but not for now.</li><li><strong>Robust</strong>, nothing kills the mood more than tracebacks.</li></ul><p>Contradicting the last point a bit, I decided to make it an exercise in quickly creating a hacky solution, then gradually iterating on it. I wasn&rsquo;t sure which ideas would work so I wanted to avoid prematurely overengineering a solution, only to hate using it.</p><h3 id=iteration-1--basic-mvp>Iteration 1 – Basic MVP<a hidden class=anchor aria-hidden=true href=#iteration-1--basic-mvp>#</a></h3><p>The first iteration was basic. I began creating a script that takes a CSV file with the following fields:</p><ul><li>A sentence with Markdown bold tags highlighting target words.</li><li>Definitions for each of the target words.</li><li>Floating point values for the start and end times of the sentence in the content.</li></ul><p><img loading=lazy src=image-5.png alt="Example CSV File"></p><blockquote><p>Example CSV file. The first line is the target Youtube video URL, and all others are extracted sentences.</p></blockquote><p>This script then generates readings all the target words using the Python library <code>pinyin</code>. Then, it uses <code>youtube-dl</code> to download the video and uses <code>ffmpeg</code> to extract audio and screenshots from the target regions. These are then formatted as another CSV file that is importable into Anki.</p><p>Although basic, this is easier than using screen capture tools to manually create audio recordings and screenshots. I am still, however, bottlenecked by copying or writing the sentences and extracting precise, sub-second timestamps.</p><h3 id=iteration-2--enter-whisper>Iteration 2 – Enter Whisper<a hidden class=anchor aria-hidden=true href=#iteration-2--enter-whisper>#</a></h3><p>Iteration 2 sought to iron out the task of creating the sentences and the timestamps. I stayed on brand and used ✨AI✨ to transcribe audio for me. Using <code>youtube-dl</code> to download the audio, I passed it to <a href=https://github.com/sanchit-gandhi/whisper-jax>Whisper JAX</a> and parsed the output into a CSV file in the same format as the one I created by hand in Iteration 1 – just with an empty definitions field.</p><p>Now all I need to do is highlight the target words and write their definition; Whisper handles the transcription and timestamps for me. If a particular sentence is too easy or too hard, I can just delete the sentence line. Highlighting and deleting like this is a breeze using Vim.</p><p>There remain some issues with this approach. For one, I found the timestamps were not granular enough, usually just to the second. Secondly, sometimes these timesteps were simply inaccurate or had glitchy repeated sentences. Finally, although on GPU it ran blazing fast, it was a slow on my laptop CPU.</p><h3 id=iteration-3--whisper-but-slower>Iteration 3 – Whisper but slower<a hidden class=anchor aria-hidden=true href=#iteration-3--whisper-but-slower>#</a></h3><p>The current iteration instead uses <a href=https://github.com/sanchit-gandhi/whisper-jax>whisper.cpp</a> – a zero dependency, optimised for CPU Whisper implementation, with limited GPU support to boot. This makes it more useable on laptop at the cost of slower desktop performance. In practice this doesn&rsquo;t matter as I can simply do something else as the script runs. Furthermore, I&rsquo;ve found the timestamps and transcriptions to be more accurate than the JAX implementation so far.</p><p><img loading=lazy src=image-7.png alt="Example whisper.cpp output with timestamps"></p><blockquote><p>Example transcript from <code>whisper.cpp</code> with timestamps on a short excerpt from JFK&rsquo;s famous speech.</p></blockquote><p>The cherry on top was installing a command line version of Google Translate <code>trans</code>. It is great for quickly checking the meaning of words by switching terminal focus, rather than using my phone dictionary. Google Translate is unreliable for long or nuanced sentences, but not bad for single words. If you, dear reader, know of a command line English-Chinese dictionary please let me know.</p><h2 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h2><p>The solution now is far better than manual work, but still not perfect. Remaining limitations include:</p><ul><li>Targets Youtube exclusively but could be extended to local videos. However, it cannot handle DRM-enabled content like on Netflix. Solution? 🏴‍️ 🤷</li><li>Some videos <em>just don&rsquo;t work</em> with Whisper. For example <a href=https://www.youtube.com/@XiaoYingFood>Xiaoying Cuisine</a>, a cooking channel, has many videos that Whisper just fails to transcribe, simply failing to print most the sentences. This happens with both implementations I tried, so it perhaps is an issue with the model itself.</li><li>Cannot stream in videos, so longer content takes a while to process. Streaming is possible, but not necessary as I can start the script and do something else while it processes. I could also chunk up the video and transcribe each chunk at a time.</li><li>Transcriptions are not 100% accurate, especially with certain accents. I wouldn&rsquo;t recommend this method unless you are advanced enough to spot errors. This slows things down as I cannot fully trust Whisper, but overall it is very accurate and faster than manual transcription.</li><li>Cannot use on mobile. This would be cool, but I know little about mobile development.</li></ul><p>The next step is to refactor the script now that I have a good proof of concept. It is currently an awful amalgamation of Python, binaries, and shell scripts. I would not add automatic definition generation. Chinese is a highly contextual, so often the meaning of words changes depending on context – on top of the nuance and ambiguity in all languages. As always, YMMV.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Although I&rsquo;ve defined a somewhat overkill method, I am quite lazy, so I know I need to proactively reduce friction in order to actually do things. This is somewhat contradictory, but it is what it is.</p><p>After using it for the past week, it is definitely helping with my consistency. Not only is there less friction, it also feels good to use a tool you yourself made. Additionally, writing about this and my goals publicly provides some soft peer pressure which should also help remain consistent. We&rsquo;ll see how that actually goes with time.</p><p>In general with computers, I feel it is nearly always worth the time to optimise your workflow in order to minimise the work required wherever possible. That is why I am so into using Linux for personal computing, but that is for another blog post entirely. This is just one instance of this ideology.</p><p>Language learning itself is a topic I could talk at length about, but that is again for another time. I actually have a draft on this that I wrote before going to Taiwan, however it was so long that I couldn&rsquo;t finish in time before leaving. This made a lot of the content totally out of date and hence is confined to a dead branch. Some day though.</p><p>I had a few goals when writing this blog post. One, to show a cool use of AI for language learning. Two, to hopefully inspire people to identify and reduce friction in their own workflows. Finally, to practice writing short(er) form content.</p><p><em>If you liked this post please consider following me on <a href=https://twitter.com/alexfmckinney>Twitter</a> or use this site&rsquo;s RSS feed for notifications on future ramblings. Alternatively you can navigate to the root of this website and repeatedly refresh until something happens. Thank you for reading this far and I hope you found it useful!</em></p><hr><h3 id=acknowledgements-and-extra-resources>Acknowledgements and Extra Resources<a hidden class=anchor aria-hidden=true href=#acknowledgements-and-extra-resources>#</a></h3><p>I take heavy inspiration from <a href=https://www.youtube.com/@Livakivi>Livakivi&rsquo;s Youtube Channel</a>. I would recommend checking it out even if you aren&rsquo;t interested in language learning. They cover many other topics like home renovation, drawing, technology, and just generally learning things. His full series on language learning can be found <a href="https://www.youtube.com/playlist?list=PLYLTtm-WITnklOP-i4NClO1qA2qTkLC7z">here</a>.</p><p><em>Found something wrong with this blog post? Let me know via email or Twitter!</em></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://afmck.in>Alex McKinney</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
<a href=/cheems rel="noopener noreferrer">🐕️</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>