<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Brief Overview of Parallelism Strategies in Deep Learning | Alex McKinney</title>
<meta name=keywords content><meta name=description content="Royal York Crescent, Bristol.
It has been nearly half a year since I started my first real job as a fully-fledged graduate. This has taken the form of being an AI Engineer at Graphcore, a Bristol-based AI accelerator startup. In quite a short amount of time, I have learned a great great deal and I am grateful for the opportunity and the patience of my colleagues – the latter of which is particularly needed when tutoring the average, fresh compsci graduate."><meta name=author content><link rel=canonical href=https://afmck.in/posts/2023-02-26-parallelism/><link crossorigin=anonymous href=/assets/css/stylesheet.min.0fbade1da18e97af7375719317bb9ec325a4f325a425cbb45b6b7c0f5ce82a5d.css integrity="sha256-D7reHaGOl69zdXGTF7uewyWk8yWkJcu0W2t8D1zoKl0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.5b9ae0304f93db6cc493f51846f012428af399c614b4f2fbdb7fa59dd4d5ef5b.js integrity="sha256-W5rgME+T22zEk/UYRvASQorzmcYUtPL723+lndTV71s=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://afmck.in/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://afmck.in/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://afmck.in/favicon-32x32.png><link rel=apple-touch-icon href=https://afmck.in/apple-touch-icon.png><link rel=mask-icon href=https://afmck.in/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:title" content="A Brief Overview of Parallelism Strategies in Deep Learning"><meta property="og:description" content="Royal York Crescent, Bristol.
It has been nearly half a year since I started my first real job as a fully-fledged graduate. This has taken the form of being an AI Engineer at Graphcore, a Bristol-based AI accelerator startup. In quite a short amount of time, I have learned a great great deal and I am grateful for the opportunity and the patience of my colleagues – the latter of which is particularly needed when tutoring the average, fresh compsci graduate."><meta property="og:type" content="article"><meta property="og:url" content="https://afmck.in/posts/2023-02-26-parallelism/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-26T09:00:00+00:00"><meta property="article:modified_time" content="2023-02-26T09:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Brief Overview of Parallelism Strategies in Deep Learning"><meta name=twitter:description content="Royal York Crescent, Bristol.
It has been nearly half a year since I started my first real job as a fully-fledged graduate. This has taken the form of being an AI Engineer at Graphcore, a Bristol-based AI accelerator startup. In quite a short amount of time, I have learned a great great deal and I am grateful for the opportunity and the patience of my colleagues – the latter of which is particularly needed when tutoring the average, fresh compsci graduate."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://afmck.in/posts/"},{"@type":"ListItem","position":3,"name":"A Brief Overview of Parallelism Strategies in Deep Learning","item":"https://afmck.in/posts/2023-02-26-parallelism/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Brief Overview of Parallelism Strategies in Deep Learning","name":"A Brief Overview of Parallelism Strategies in Deep Learning","description":"Royal York Crescent, Bristol.\nIt has been nearly half a year since I started my first real job as a fully-fledged graduate. This has taken the form of being an AI Engineer at Graphcore, a Bristol-based AI accelerator startup. In quite a short amount of time, I have learned a great great deal and I am grateful for the opportunity and the patience of my colleagues – the latter of which is particularly needed when tutoring the average, fresh compsci graduate.","keywords":[],"articleBody":" Royal York Crescent, Bristol.\nIt has been nearly half a year since I started my first real job as a fully-fledged graduate. This has taken the form of being an AI Engineer at Graphcore, a Bristol-based AI accelerator startup. In quite a short amount of time, I have learned a great great deal and I am grateful for the opportunity and the patience of my colleagues – the latter of which is particularly needed when tutoring the average, fresh compsci graduate.\nChief among what I have learned is a wide array of parallelism strategies. If you are at least somewhat familiar with Graphcore’s IPU accelerators, you will know why. But for the uninitiated, the amount of on-chip memory that can be directly accessed without hassle on an IPU, is considerably smaller than the GPUs of today. Luckily, it has a substantial amount of secondary DRAM memory - also on IPU - so offloading is faster than it would be to CPU. Nevertheless, though the IPU has a number of advantages versus a GPU, the reduced size does mean that resorting to parallelism is more common.\nThough that doesn’t sound great, if you stop to consider the fast growing model sizes in the deep learning community, you will maybe notice that this isn’t a huge deal if you are interested in large models. Why is that? Because now mass parallelism is also required on GPUs. Huge scales are the great equaliser it seems, and a single accelerator of any type can do nought next to the likes of GPT-3, PaLM, Parti, and friends.\nThis is exactly the position I found myself in when I landed in Graphcore, aligning myself quickly on the large model side of things. Conceptually, parallelism in deep learning is not tricky, but regarding implementations I still have a ways to go. For now though, I will share what I have learned on this topic.\nLuckily, the concepts are agnostic to the type of compute device used – you could even treat everything as a desktop CPU, or a graphing calculator – so this will not be IPU specific nor even framework specific, only high-level concepts. This means I will not include code examples, but I will link to tutorials and frameworks that implement the concepts I discuss at the end of each section.\nWorking with a Single Device It’s helpful to begin with the single device case. One host (a regular PC with a CPU and memory), one accelerator we want to run our model on, suppose a GPU, which has its own processing and memory components. During execution, we load parameters and data onto the device, and create intermediate data on-device such as activations and gradients. If you are a deep learning practioner you should be familiar with this scenario.\nYou will also be familiar with this other scenario:\n\u003e x = torch.empty(int(1e12), device=device) OutOfMemoryError: CUDA out of memory. Tried to allocate 3725.29 GiB (GPU 0; 23.69 GiB total capacity; 3.73 GiB already allocated; 18.56 GiB free; 3.75 GiB reserved in total by PyTorch) If reserved memory is \u003e\u003e allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF For which you probably have a few solutions you immediately reach for, such as a credit card.\nOn-device memory is typically used for the following things:\nStoring model parameters. Storing activations. Storing gradients. Storing optimiser states. Storing code. Having to store gradients and optimiser states on-device is one reason why training models uses more memory than just running inference. This is particularly true for more advanced (read: memory-hungry) optimisers like Adam.\nAnd potential solutions include the following:\nSwitch to a lower floating point precision :right_arrow: less bits used per tensor. Decrease micro-batch size and compensate with gradient accumulation :right_arrow: reduces size of activations and gradients. Turn on “no gradient” or “inference mode” :right_arrow: only store current activations, no gradients stored. Clearly inference only. Sacrifice extra compute for memory savings (ex: recomputation of activations, attention serialisation) CPU offload :right_arrow: only load tensors onto device when needed, otherwise store in host memory. ..among others!\nThese solutions help a lot in most workflows and can allow for training much larger models on a single device than you would expect (see this Microsoft blog). However, for one reason or another, this is sometimes just not practical or has other downsides such as training instability. In such cases, resorting to parallelism may be necessary.\nData Parallelism Arguably one of the simplest forms of parallelism, data parallelism simply takes all the devices you have, copies model and optimiser parameters and code onto all of them, then sends different micro-batches to each device. Gradients are then synchronised between devices, followed by an optimiser step, resulting in parameter updates. In essence, we use more devices to chew through the dataset faster, thus obtaining a speedup. It also enables larger (global) batch sizes without performing too many gradient accumulation iterations.\nNote, that the model and optimiser parameters are replicated on all devices (all replicas). Hence, in order for this parallelism to work, we need to be able to fit the entire model with a micro-batch size of at least one on a single replica.\nData parallelism won’t directly help fit larger models, but can help increase the batch size that you likely had to reduce to squeeze the model in originally. My first encounter with this was during my master’s dissertation, training a VQ-GAN model, where I could only train on a single device with a micro-batch size of one, but could obtain a global batch size of 16 with 4 devices and 4 gradient accumulation steps.\nData parallelism is also relatively communication light, as we only need to all-reduce the gradients whenever we update the optimiser, which could be infrequent depending on the number of gradient accumulation steps. This also reduces the need for high-speed interconnect between replicas, as communication is infrequent.\nTo use data parallelism in your code see this post for PyTorch fans and this post for Jax. I also like this post by Kevin Kai-chuang for PyTorch. However, by far the easiest way to add data parallelism to your code in PyTorch is to use Huggingface Accelerate which near-perfectly abstracts away data parallelism, and integrates with powerful libraries like Deepspeed, right out of the box.\nPipeline Parallelism Pipeline Parallelism is used when the entire model is too large to fit on a single device. The model is split into stages and each stage executed on a different device, passing activations between neighbouring stages when a boundary is reached during execution.\nFor inference, this is extremely good for improving total throughput of the system as multiple micro-batches can be processed in parallel, with communication only happening at stage boundaries. Sometimes, even if the model fits on one device, pipelining can be used to improve throughput like this (at the cost of latency).\nShamelessly taken from a tutorial from my job. Same goes for all images in this section on pipelining.\nThere is a noticeable gap at the start of the above inference pipeline. This is known as the ramp-up phase where device utilisation cannot be at 100%. Clearly stage 3 cannot begin executing micro-batch 1 until stage 1 and 2 are both done with micro-batch 1. However, in this scheme, once the final stage $N$ has received the first micro-batch, we reach the main phase and reach maximum utilisation. Once the first stage processes the final micro-batch, we reach the ramp-down stage, as their is simply no more data left to execute on.\nFor best utilisation, the time to execute each stage should be as balanced as possible. This is because if one stage finishes early, it may have to wait for the subsequent stage to be ready before passing on its activations and executing its next micro-batch. This is easy in relatively homogeneous models like transformers, where most layers are typically the same size, but difficult in other models such as Resnets or UNets.\nHow does this extend to training?\nIt makes sense for each stage $t$ to also handle its own backwards pass $t$, so we don’t need to copy activations or parameters to multiple devices. However, one problem is that we cannot handle the backwards for a stage $t$, without having the results for backwards $t+1, \\dots, N$ and the forwards for $t-1, \\dots, 1$.\nLet’s begin with the simplest scheme that meets these conditions: Yes, the second set for forward should probably say “B2”.\nIn this scheme, only one micro-batch is in play at a time, leaving all stages but one idle. After finishing the final stage, we turn around and begin the backwards pass, again leaving all but one stage idle. Once a full batch is computed (in other words, after the ramp-down), a weight update is performed. This means the utilisation is always going to be (at most) $1/N$ of full utilisation. Clearly extremely inefficient!\nHowever, good for debugging purpose!\nWe can do better with a grouped pipeline: The ramp-up phase almost consists of two “sub-ramp-ups”: the first being the same as the inference ramp-up, followed by a ramp-up of the backwards passes which alternate with main-phase forward passes. Once the main-stage is reached, we alternate between forward and backwards passes on all stages.\nWe can alternate as once a stage $t$ processes the backwards of micro-batch $b$, it can discard activations and accumulate gradients. It is then ready to process the next forward pass for micro-batch $b+1$. Like before, a weight update occurs after a ramp-down phase.\nAnother approach is to interleave the forwards and backwards passes: At any given point, half the stages are executing a forward pass, and half a backwards pass. Because of this, the ramp-up and ramp-down phases are much shorter, resulting in a quicker time to maximum utilisation.\nThe last two schemes have different advantages and disadvantages:\nAt any given time, the grouped scheme executes twice as much mini-batches as the interleaved scheme, meaning more memory is required to store activations Grouped schemes executes all forward and backwards together, meaning communication is less frequent. Interleaved executes separately, resulting in more communication and also some idle time when forward passes wait for backward passes – which typically take longer than forward passes. Hence, grouped schemes are typically faster than interleaved. Interleaved ramp-up and ramp-down time is about twice as fast as grouped, meaning it is quicker to reach full utilisation. Pipeline parallelism uses much more communication than data parallel, however less than tensor parallelism, which I will discuss in the next section. Communication is limited to boundaries between stages, meaning regardless of the number of stages, each one will send one set of activations, and receive one set. The communication can be done in parallel between all replicas.\nPyTorch has native support for pipelining here which uses the GPipe algorithm. This was used to train larger transformer models in this tutorial. Deepspeed provides more advanced pipeline parallelism, which is explained in this tutorial.\nJax, meanwhile, has no native support for pipeline parallelism, but frameworks around Jax such as Alpa do support it, demonstrated in this tutorial. Lack of native support in Jax likely comes from its emphasis on TPUs, which have high bandwidth interconnect between all replicas in a TPU pod (versus GPUs which have fast interconnect only within a host). This means it is preferable to simply use data and tensor parallelism (more on the latter later) so to avoid pipeline bubbles.\nTensor Parallelism What about if a single layer is too big to fit on a single replica? Say for example, a particularly large MLP expansion, expensive self-attention layer, or a large embedding layer. In such cases, the parameters of a layer can be split across multiple devices. Partial results are then computed on each device before materialising the final result by communicating between the replicas.\nTake, for example, the MLP block in a standard transformer model that projects a vector $x$ to and from a space of dimension $d$ and $4d$:\n$$h = W_2 \\cdot f(W_1 \\cdot x + b_1) + b_2 $$ where $f$ is a nonlinear activation function, $W_{*}$ are the weight matrices, and $b_{*}$ are the bias vectors.\nTo turn this into a tensor parallel layer, do the following:\nSplit $W_1$ row-wise into $n$ pieces, sending one to each of $n$ replicas. Split $b_1$ into $n$ pieces, as above. Split $W_2$ column-wise into $n$ pieces, sending one to each of $n$ replicas. Then, given an input $x$, do on each replica $i$:\nCompute $f(W_1^{(i)} \\cdot x + b_1^{(i)})$, resulting in a vector $z_i$ of size $4d/n$. Compute $W_2^{(i)} \\cdot z_i$, resulting in $\\hat{h_i}$ of size $d$ This, naturally, does not give the same result. The next part resolves this:\nCommunicate between replicas to compute $\\sum^n_{i=1} \\hat{h_i} $ On all replicas add $b_2$ (which is identical on all replicas), to get the final result $h$ on all replicas. The first operation sums the partial results across all replicas, which means all replicas have the same value at this point. Then, all replicas identically compute $b_2$, which still results in the same (and correct) result, despite no communication happening after this addition.\nA nice exercise is to write out mathematically the operations happening here, and see that we do indeed arrive at the same result. However, I will save myself you the pain here.\nA diagram of a tensor parallel MLP from the Megatron paper.\nAnother example is an embedding layer in a transformer. We can split the embedding matrix along the vocabulary dimension, resulting in a shards of shape $V/n \\times d$. All replicas receive the same input tokens and compute the embedding. However, if any given token falls outside a given replica’s valid range, a zero tensor is instead returned. Then, an all-reduce will rematerialise the final result, as only one replica (for any given element in the input sequence) will have a non-zero tensor.\nOther examples include splitting attention heads, convolution channels or even the spatial dimensions themselves, between the replicas.\nRegardless of the context tensor parallelism is applied, it should be noted that communication between replicas in the same tensor parallel group occur much more frequently than in data parallel groups or between pipeline stages. This means that time spent communicating compared to actually computing a result increases. Because of this, replicas in the same tensor parallel group should be placed on higher bandwidth interconnect, if possible.\nDepending on the model size tensor parallelism can be totally unavoidable, but should be avoided if possible! Of course, exceptions are also possible. One case is where (suppose) you could not use pipeline parallelism, so in order to fit the model onto the available devices, you use tensor parallelism throughout the model, despite no single layer causing an out of memory. This is somewhat orthogonal to pipeline parallelism: splitting through the model rather than across.\nSupport for tensor parallelism in PyTorch is experimental see this RFC but is supported in Deepspeed as “tensor-slicing model-parallelism”. Tensor parallelism can be achieved in Jax using jax.pjit.\nAll together now.. A keen eyed reader may have noticed that these parallelism strategies should not be incompatible with one another – perhaps they may even use the term orthogonal. Indeed, when we target very large model sizes, or have a lot of compute to throw around, we start arranging replicas into hierarchies and groups.\nTake for example, a system with a data parallel factor of $n_d$. Each data parallel instance will contain an exact copy of the model – simply sending different batches to each data parallel instance. If we employ pipeline parallelism with $n_p$ pipeline stages, and each data parallel instance contains an exact copy, then each data parallel instance must also contain $n_p$ pipeline stages, giving $n_p \\cdot n_d$ replicas total.\nAdding tensor parallelism to the mix, splitting layers among $n_t$ replicas, each pipeline stage will use $n_t$ replicas. Naturally, this gives a total number of replicas of $n = n_p \\cdot n_d \\cdot n_t$. This gives a hierarchy of data parallel at the very top, down to tensor parallelism at the bottom. This also translates near perfectly to a typical compute cluster.\nRecall that data parallel communications (the gradient all reduce) occur much less frequently than tensor parallelism communication (potentially multiple times per layer). Furthermore, larger clusters typically consist of multiple nodes, where communication between devices on a single node is much faster than communication between nodes. It therefore makes sense then to group tensor parallel replicas that need to communicate together on a single node, and place ones that do not across nodes. In other words, prioritise placing tensor parallel groups on high-bandwidth interconnect over data parallel groups.\nFor example, using IPUs, certain IPUs have more links between them and so give a higher speed interconnect. Moreover, certain IPUs exist together on the same motherboard, whereas others only share the same host server, or perhaps even use different host servers entirely. For GPUs, perhaps certain GPUs are connected together using NVLink and others over ethernet between different servers - perhaps even over the internet in a swarm-like style.\nThough these parallelism methods are orthogonal to one another, it is still a challenging task to combine them. Moreover, though a certain scheme may make the model fit, it may be terribly inefficient. The problem of taking a model and cluster configuration as input and producing a good grouping of replicas to maximise performance is an art in and of itself. It remains still an active area of research. One such method is Alpa by Google research which has a framework built around Jax here.\nConclusion Unless something tremendously dramatic happens in deep learning research, very large models are here to stay. Therefore, at least being aware of these techniques will become a fact of life for most AI engineers. Even if the interfaces become very clean in the future (so engineers can just fire-and-forget) there will be still a benefit in knowing how things word under the hood, just like the benefits gained from studying anything we take for granted now.\nI am still a beginner in this topic and had never deeply explored before starting my job. Luckily for me I found it very interesting, albeit a little annoying to work with versus working with a single device. There is also a bit of a cognitive shift required to move from a serial execution world, where most people start from, to a massively parallel world, where we are going. Same goes for simply handling such large models: from longer load times to slower development loops to simply comprehending that “if we split the parameters of Bloom among every human being, each one would get 22 parameters”. However, the difficulty is worth it to better understand how large behemoths like GPT-3 are orchestrated.\nIn the future and as I gain more experience, I would like to dive into further detail about specific methods mentioned here and those yet-to-come. However for now, and given my current experience level, I simply hope this post can help you gain a high-level understanding of the methods currently available to us in deep learning system parallelism.\n","wordCount":"3186","inLanguage":"en","datePublished":"2023-02-26T09:00:00Z","dateModified":"2023-02-26T09:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://afmck.in/posts/2023-02-26-parallelism/"},"publisher":{"@type":"Organization","name":"Alex McKinney","logo":{"@type":"ImageObject","url":"https://afmck.in/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://afmck.in accesskey=h title="Alex McKinney (Alt + H)">Alex McKinney</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://afmck.in/about title=About><span>About</span></a></li><li><a href=https://afmck.in/posts title=Posts><span>Posts</span></a></li><li><a href=https://afmck.in/misc title=Misc><span>Misc</span></a></li><li><a href=https://afmck.in/cv.pdf title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>A Brief Overview of Parallelism Strategies in Deep Learning</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#working-with-a-single-device aria-label="Working with a Single Device">Working with a Single Device</a></li><li><a href=#data-parallelism aria-label="Data Parallelism">Data Parallelism</a></li><li><a href=#pipeline-parallelism aria-label="Pipeline Parallelism">Pipeline Parallelism</a></li><li><a href=#tensor-parallelism aria-label="Tensor Parallelism">Tensor Parallelism</a></li><li><a href=#all-together-now aria-label="All together now..">All together now..</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p><img loading=lazy src=img/header.jpg alt></p><blockquote><p><em>Royal York Crescent, Bristol.</em></p></blockquote><p>It has been nearly half a year since I started my first real job as a
fully-fledged graduate. This has taken the form of being an AI Engineer at
Graphcore, a Bristol-based AI accelerator startup. In quite a short amount of time, I
have learned a great great deal and I am grateful for the opportunity and the
patience of my colleagues – the latter of which is particularly needed when
tutoring the average, fresh compsci graduate.</p><p>Chief among what I have learned is a wide array of parallelism strategies. If
you are at least somewhat familiar with Graphcore&rsquo;s <strong>IPU</strong> accelerators, you
will know why. But for the uninitiated, the amount of on-chip memory that can
be directly accessed without hassle on an IPU, is considerably smaller than the
GPUs of today. Luckily, it has a substantial amount of secondary DRAM memory -
also on IPU - so offloading is faster than it would be to CPU. Nevertheless,
though the IPU has a number of advantages versus a GPU, the reduced size
does mean that resorting to parallelism is more common.</p><p>Though that doesn&rsquo;t sound great, if you stop to consider the fast growing model
sizes in the deep learning community, you will maybe notice that this isn&rsquo;t a
<em>huge deal</em> if you are interested in large models. Why is that? Because now mass
parallelism is also required on GPUs. Huge scales are the great equaliser it
seems, and a single accelerator of any type can do nought next to the likes of
GPT-3, PaLM, Parti, and friends.</p><p>This is exactly the position I found myself in when I landed in Graphcore,
aligning myself quickly on the large model side of things. Conceptually,
parallelism in deep learning is not tricky, but regarding implementations I
still have a ways to go. For now though, I will share what I have learned on
this topic.</p><p>Luckily, the concepts are agnostic to the type of compute device used – you
could even treat everything as a desktop CPU, or a graphing calculator – so this
will not be IPU specific nor even framework specific, only high-level concepts.
This means I will not include code examples, but I will link to tutorials and
frameworks that implement the concepts I discuss at the end of each section.</p><h3 id=working-with-a-single-device>Working with a Single Device<a hidden class=anchor aria-hidden=true href=#working-with-a-single-device>#</a></h3><p>It&rsquo;s helpful to begin with the single device case. One host (a regular PC with
a CPU and memory), one accelerator we want to run our model on, suppose a GPU,
which has its own processing and memory components. During execution, we load
parameters and data onto the device, and create intermediate data on-device
such as activations and gradients. If you are a deep learning practioner you
should be familiar with this scenario.</p><p>You will also be familiar with this other scenario:</p><pre tabindex=0><code>&gt; x = torch.empty(int(1e12), device=device)
OutOfMemoryError: CUDA out of memory. Tried to allocate 3725.29 GiB (GPU 0;
23.69 GiB total capacity; 3.73 GiB already allocated; 18.56 GiB free; 3.75 GiB
reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try
setting max_split_size_mb to avoid fragmentation.  See documentation for Memory
Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre><p>For which you probably have a few solutions you immediately reach for, such as a credit card.</p><p>On-device memory is typically used for the following things:</p><ul><li>Storing model parameters.</li><li>Storing activations.</li><li>Storing gradients.</li><li>Storing optimiser states.</li><li>Storing code.</li></ul><blockquote><p>Having to store gradients and optimiser states on-device is one reason why
training models uses more memory than just running inference. This is
particularly true for more advanced (read: memory-hungry) optimisers like
Adam.</p></blockquote><p>And potential solutions include the following:</p><ul><li>Switch to a lower floating point precision :right_arrow: less bits used per
tensor.</li><li>Decrease micro-batch size and compensate with gradient accumulation
:right_arrow: reduces size of activations and gradients.</li><li>Turn on &ldquo;no gradient&rdquo; or &ldquo;inference mode&rdquo; :right_arrow: only store current
activations, no gradients stored. Clearly inference only.</li><li>Sacrifice extra compute for memory savings (ex: recomputation of activations,
attention serialisation)</li><li>CPU offload :right_arrow: only load tensors onto device when needed,
otherwise store in host memory.</li></ul><p>..among others!</p><p>These solutions help a lot in most workflows and can allow for training much
larger models on a single device than you would expect (see <a href=https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/>this Microsoft
blog</a>).
However, for one reason or another, this is sometimes just not practical or has
other downsides such as training instability. In such cases, resorting to
parallelism may be necessary.</p><h3 id=data-parallelism>Data Parallelism<a hidden class=anchor aria-hidden=true href=#data-parallelism>#</a></h3><p>Arguably one of the simplest forms of parallelism, data parallelism simply takes
all the devices you have, copies model and optimiser parameters and code onto
all of them, then sends different micro-batches to each device. Gradients are
then synchronised between devices, followed by an optimiser step, resulting in
parameter updates. In essence, we use more devices to chew through the dataset
faster, thus obtaining a speedup. It also enables larger (global) batch sizes
without performing too many gradient accumulation iterations.</p><p>Note, that the model and optimiser parameters are replicated on all
devices (all replicas). Hence, in order for this parallelism to work, we need to
be able to fit the entire model with a micro-batch size of at least one on a
single replica.</p><p>Data parallelism won&rsquo;t directly help fit larger models, but can help increase
the batch size that you likely had to reduce to squeeze the model in
originally. My first encounter with this was during my master&rsquo;s dissertation,
training a VQ-GAN model, where I could only train on a single device with a
micro-batch size of one, but could obtain a global batch size of 16 with 4
devices and 4 gradient accumulation steps.</p><p>Data parallelism is also relatively communication light, as we only need to
all-reduce the gradients whenever we update the optimiser, which could be
infrequent depending on the number of gradient accumulation steps. This also
reduces the need for high-speed interconnect between replicas, as communication
is infrequent.</p><p>To use data parallelism in your code see <a href=https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>this post for PyTorch
fans</a> and <a href=https://www.mishalaskin.com/posts/data_parallel>this
post for Jax</a>. I also like
<a href=https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html>this post by Kevin
Kai-chuang</a>
for PyTorch. However, by far the easiest way to add data parallelism to your
code in PyTorch is to use <a href=https://github.com/huggingface/accelerate>Huggingface
Accelerate</a> which near-perfectly
abstracts away data parallelism, and integrates with powerful libraries like
Deepspeed, right out of the box.</p><h3 id=pipeline-parallelism>Pipeline Parallelism<a hidden class=anchor aria-hidden=true href=#pipeline-parallelism>#</a></h3><p>Pipeline Parallelism is used when the entire model is too large to fit on a
single device. The model is split into stages and each stage executed on a
different device, passing activations between neighbouring stages when a
boundary is reached during execution.</p><p>For inference, this is extremely good for improving total throughput of the
system as multiple micro-batches can be processed in parallel, with communication only
happening at stage boundaries. Sometimes, even if the model fits on one device,
pipelining can be used to improve throughput like this (at the cost of latency).</p><p><img loading=lazy src=img/pipeline-inference.png alt="Inference pipeline"></p><blockquote><p>Shamelessly taken from <a href=https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html>a tutorial from my job.</a> Same goes for all images in this section on pipelining.</p></blockquote><p>There is a noticeable <strong>gap</strong> at the start of the above inference pipeline. This
is known as the <strong>ramp-up</strong> phase where device utilisation cannot be at 100%.
Clearly stage 3 cannot begin executing micro-batch 1 until stage 1 and 2 are
both done with micro-batch 1. However, in this scheme, once the final stage $N$ has
received the first micro-batch, we reach the <strong>main phase</strong> and reach maximum
utilisation. Once the first stage processes the final micro-batch, we reach the
<strong>ramp-down</strong> stage, as their is simply no more data left to execute on.</p><p>For best utilisation, the time to execute each stage should be as balanced as
possible. This is because if one stage finishes early, it may have to wait for
the subsequent stage to be ready before passing on its activations and executing
its next micro-batch. This is easy in relatively homogeneous models like transformers,
where most layers are typically the same size, but difficult in other models
such as Resnets or UNets.</p><p>How does this extend to training?</p><p>It makes sense for each stage $t$ to also handle its own backwards pass $t$, so
we don&rsquo;t need to copy activations or parameters to multiple devices. However, one
problem is that we cannot handle the backwards for a stage $t$, without having
the results for backwards $t+1, \dots, N$ and the forwards for $t-1, \dots, 1$.</p><p>Let&rsquo;s begin with the simplest scheme that meets these conditions:
<img loading=lazy src=img/pipeline-sequential.png alt="Sequential pipeline"></p><blockquote><p>Yes, the second set for forward should probably say &ldquo;B2&rdquo;.</p></blockquote><p>In this scheme, only one micro-batch is in play at a time, leaving all stages
but one idle. After finishing the final stage, we turn around and begin the
backwards pass, again leaving all but one stage idle. Once a full batch is
computed (in other words, after the ramp-down), a weight update is performed.
This means the utilisation is always going to be (at most) $1/N$ of full
utilisation. Clearly extremely inefficient!</p><blockquote><p>However, good for debugging purpose!</p></blockquote><p>We can do better with a <strong>grouped pipeline:</strong>
<img loading=lazy src=img/pipeline-grouped.png alt="Grouped pipeline"></p><p>The ramp-up phase almost consists of two &ldquo;sub-ramp-ups&rdquo;: the first being the
same as the inference ramp-up, followed by a ramp-up of the backwards passes
which alternate with main-phase forward passes. Once the main-stage is reached,
we alternate between forward and backwards passes on all stages.</p><p>We can alternate as once a stage $t$ processes the backwards of micro-batch $b$,
it can discard activations and accumulate gradients. It is then ready to process
the next forward pass for micro-batch $b+1$. Like before, a weight update occurs
after a ramp-down phase.</p><p>Another approach is to interleave the forwards and backwards passes:
<img loading=lazy src=img/pipeline-interleaved.png alt="Interleaved pipeline"></p><p>At any given point, half the stages are executing a forward pass, and half a backwards pass. Because of this, the ramp-up and ramp-down phases are much shorter, resulting in a quicker time to maximum utilisation.</p><p>The last two schemes have different advantages and disadvantages:</p><ul><li>At any given time, the grouped scheme executes twice as much mini-batches as
the interleaved scheme, meaning more memory is required to store activations</li><li>Grouped schemes executes all forward and backwards together, meaning
communication is less frequent. Interleaved executes separately, resulting in
more communication and also some idle time when forward passes wait for backward
passes – which typically take longer than forward passes. Hence, grouped schemes
are typically faster than interleaved.</li><li>Interleaved ramp-up and ramp-down time is about twice as fast as grouped,
meaning it is quicker to reach full utilisation.</li></ul><p>Pipeline parallelism uses much more communication than data parallel, however
less than tensor parallelism, which I will discuss in the next section.
Communication is limited to boundaries between stages, meaning regardless of the
number of stages, each one will send one set of activations, and receive one
set. The communication can be done in parallel between all replicas.</p><p>PyTorch has native support for pipelining
<a href=https://pytorch.org/docs/stable/pipeline.html>here</a> which uses the <a href=https://arxiv.org/abs/1811.06965>GPipe
algorithm</a>. This was used to train larger
transformer models in <a href=https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html>this
tutorial</a>.
Deepspeed provides more advanced pipeline parallelism, which is explained <a href=https://www.deepspeed.ai/tutorials/pipeline/>in
this tutorial</a>.</p><p>Jax, meanwhile, has no native support for pipeline parallelism, but frameworks
around Jax such as Alpa do support it, demonstrated in <a href=https://alpa.ai/tutorials/pipeshard_parallelism.html>this
tutorial</a>. Lack of native
support in Jax likely comes from its emphasis on TPUs, which have high bandwidth
interconnect between all replicas in a TPU pod (versus GPUs which have fast
interconnect only within a host). This means it is preferable to simply use data
and tensor parallelism (more on the latter later) so to avoid pipeline bubbles.</p><h3 id=tensor-parallelism>Tensor Parallelism<a hidden class=anchor aria-hidden=true href=#tensor-parallelism>#</a></h3><p>What about if a single <em>layer</em> is too big to fit on a single replica? Say for
example, a particularly large MLP expansion, expensive self-attention layer, or
a large embedding layer. In such cases, the parameters of a layer can be split
across multiple devices. Partial results are then computed on each device before
materialising the final result by communicating between the replicas.</p><p>Take, for example, the MLP block in a standard transformer model that projects a
vector $x$ to and from a space of dimension $d$ and $4d$:</p><p>$$h = W_2 \cdot f(W_1 \cdot x + b_1) + b_2 $$
where $f$ is a nonlinear activation function, $W_{*}$ are the weight matrices, and
$b_{*}$ are the bias vectors.</p><p>To turn this into a tensor parallel layer, do the following:</p><ul><li>Split $W_1$ <strong>row</strong>-wise into $n$ pieces, sending one to each of $n$ replicas.</li><li>Split $b_1$ into $n$ pieces, as above.</li><li>Split $W_2$ <strong>column</strong>-wise into $n$ pieces, sending one to each of $n$ replicas.</li></ul><p>Then, given an input $x$, do on each replica $i$:</p><ul><li>Compute $f(W_1^{(i)} \cdot x + b_1^{(i)})$, resulting in a vector $z_i$ of size $4d/n$.</li><li>Compute $W_2^{(i)} \cdot z_i$, resulting in $\hat{h_i}$ of size $d$</li></ul><p>This, naturally, does not give the same result. The next part resolves this:</p><ul><li>Communicate between replicas to compute $\sum^n_{i=1} \hat{h_i} $</li><li>On all replicas add $b_2$ (which is identical on all replicas), to get the
final result $h$ on all replicas.</li></ul><p>The first operation sums the partial results across all replicas, which means
all replicas have the same value at this point. Then, all replicas identically
compute $b_2$, which still results in the same (and correct) result, despite no
communication happening after this addition.</p><p>A nice exercise is to write out mathematically the operations happening here,
and see that we do indeed arrive at the same result. However, I will save myself
<del>you</del> the pain here.</p><p><img loading=lazy src=img/tensor-parallel-mlp.png alt="Tensor parallelism of a transformer MLP"></p><blockquote><p>A diagram of a tensor parallel MLP from the Megatron paper.</p></blockquote><p>Another example is an embedding layer in a transformer. We can split the
embedding matrix along the vocabulary dimension, resulting in a shards of shape
$V/n \times d$. All replicas receive the same input tokens and compute the
embedding. However, if any given token falls outside a given replica&rsquo;s valid
range, a zero tensor is instead returned. Then, an all-reduce will rematerialise
the final result, as only one replica (for any given element in the input
sequence) will have a non-zero tensor.</p><p>Other examples include splitting attention heads, convolution channels or even
the spatial dimensions themselves, between the replicas.</p><p>Regardless of the context tensor parallelism is applied, it should be noted that
communication between replicas in the same tensor parallel group occur much more
frequently than in data parallel groups or between pipeline stages. This means
that time spent communicating compared to actually computing a result increases.
Because of this, replicas in the same tensor parallel group should be placed on
higher bandwidth interconnect, if possible.</p><p>Depending on the model size tensor parallelism can be totally unavoidable, but
should be avoided if possible! Of course, exceptions are also possible. One case
is where (suppose) you could not use pipeline parallelism, so in order to fit
the model onto the available devices, you use tensor parallelism
throughout the model, despite no single layer causing an out of memory. This is
somewhat orthogonal to pipeline parallelism: splitting through the model rather
than across.</p><p>Support for tensor parallelism in PyTorch is <strong>experimental</strong> <a href=https://github.com/pytorch/pytorch/issues/88838>see this
RFC</a> but is supported in
Deepspeed as &ldquo;tensor-slicing model-parallelism&rdquo;. Tensor parallelism can be
achieved in Jax using <a href=https://irhum.github.io/blog/pjit/><code>jax.pjit</code></a>.</p><h3 id=all-together-now>All together now..<a hidden class=anchor aria-hidden=true href=#all-together-now>#</a></h3><p>A keen eyed reader may have noticed that these parallelism strategies should not
be incompatible with one another – perhaps they may even use the term
<strong>orthogonal</strong>. Indeed, when we target very large model sizes, or have
a lot of compute to throw around, we start arranging replicas into hierarchies
and groups.</p><p><img loading=lazy src=img/3d-parallelism.png alt="Diagram from Microsoft Research&amp;rsquo;s DeepSpeed, showing how different times of parallelism can coexist and complement one another"></p><p>Take for example, a system with a data parallel factor of $n_d$. Each data
parallel instance will contain an exact copy of the model – simply sending
different batches to each data parallel instance. If we employ pipeline
parallelism with $n_p$ pipeline stages, and each data parallel instance contains
an exact copy, then each data parallel instance must also contain $n_p$ pipeline
stages, giving $n_p \cdot n_d$ replicas total.</p><p>Adding tensor parallelism to the mix, splitting layers among $n_t$ replicas,
each pipeline stage will use $n_t$ replicas. Naturally, this gives a total
number of replicas of $n = n_p \cdot n_d \cdot n_t$. This gives a hierarchy of
data parallel at the very top, down to tensor parallelism at the bottom. This
also translates near perfectly to a typical compute cluster.</p><p>Recall that data parallel communications (the gradient all reduce) occur much
less frequently than tensor parallelism communication (potentially multiple
times per layer). Furthermore, larger clusters typically consist of multiple
nodes, where communication between devices on a single node is much faster than
communication between nodes. It therefore makes sense then to group tensor
parallel replicas that need to communicate together on a single node, and place
ones that do not across nodes. In other words, prioritise placing tensor
parallel groups on high-bandwidth interconnect over data parallel groups.</p><p>For example, using IPUs, certain IPUs have more links between them and so give a
higher speed interconnect. Moreover, certain IPUs exist together on the same
motherboard, whereas others only share the same host server, or perhaps even use
different host servers entirely. For GPUs, perhaps certain GPUs are connected
together using NVLink and others over ethernet between different servers -
perhaps even <a href=https://github.com/bigscience-workshop/petals>over the internet</a>
in a swarm-like style.</p><p>Though these parallelism methods are orthogonal to one another, it is still
a challenging task to combine them. Moreover, though a certain scheme may
make the model <em>fit</em>, it may be terribly inefficient. The problem of
taking a model and cluster configuration as input and producing a good grouping
of replicas to maximise performance is an art in and of itself. It remains
still an active area of research. One such method is
<a href=https://arxiv.org/abs/2201.12023>Alpa</a> by Google research which has
a framework built around Jax <a href=https://github.com/alpa-projects/alpa>here</a>.</p><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>Unless something tremendously dramatic happens in deep learning research, very
large models are here to stay. Therefore, at least being aware of these
techniques will become a fact of life for most AI engineers. Even if the
interfaces become very clean in the future (so engineers can just
fire-and-forget) there will be still a benefit in knowing how things word under
the hood, just like the benefits gained from studying anything we take for
granted now.</p><p>I am still a beginner in this topic and had never deeply explored before
starting my job. Luckily for me I found it very interesting, albeit a little
annoying to work with versus working with a single device. There is also a bit
of a cognitive shift required to move from a serial execution world, where most
people start from, to a massively parallel world, where we are going. Same goes
for simply handling such large models: from longer load times to slower
development loops to simply comprehending that &ldquo;if we split the parameters of
Bloom among every human being, each one would get 22 parameters&rdquo;. However, the
difficulty is worth it to better understand how large behemoths like GPT-3 are
orchestrated.</p><p>In the future and as I gain more experience, I would like to dive into further
detail about specific methods mentioned here and those yet-to-come. However for
now, and given my current experience level, I simply hope this post can help you
gain a high-level understanding of the methods currently available to us in deep
learning system parallelism.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://afmck.in>Alex McKinney</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
<a href=/cheems rel="noopener noreferrer">🐕️</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>