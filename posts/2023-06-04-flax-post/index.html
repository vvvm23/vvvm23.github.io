<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Easy JAX training loops with Flax and Optax | Alex McKinney</title><meta name=keywords content><meta name=description content="In my previous blog post, I discussed JAX - a framework for high performance numerical computing and machine learning - in an atypical manner. I didn&rsquo;t create a single training loop, and only showed a couple patterns that looked vaguely machine learning-like. If you haven&rsquo;t read that blog post yet, you can read it here.
This approach was deliberate as I felt that JAX - although designed for machine learning research - is more general-purpose than that."><meta name=author content><link rel=canonical href=https://afmck.in/posts/2023-06-04-flax-post/><link crossorigin=anonymous href=/assets/css/stylesheet.min.0fbade1da18e97af7375719317bb9ec325a4f325a425cbb45b6b7c0f5ce82a5d.css integrity="sha256-D7reHaGOl69zdXGTF7uewyWk8yWkJcu0W2t8D1zoKl0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://afmck.in/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://afmck.in/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://afmck.in/favicon-32x32.png><link rel=apple-touch-icon href=https://afmck.in/apple-touch-icon.png><link rel=mask-icon href=https://afmck.in/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Easy JAX training loops with Flax and Optax"><meta property="og:description" content="In my previous blog post, I discussed JAX - a framework for high performance numerical computing and machine learning - in an atypical manner. I didn&rsquo;t create a single training loop, and only showed a couple patterns that looked vaguely machine learning-like. If you haven&rsquo;t read that blog post yet, you can read it here.
This approach was deliberate as I felt that JAX - although designed for machine learning research - is more general-purpose than that."><meta property="og:type" content="article"><meta property="og:url" content="https://afmck.in/posts/2023-06-04-flax-post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-04T12:00:00+01:00"><meta property="article:modified_time" content="2023-06-04T12:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Easy JAX training loops with Flax and Optax"><meta name=twitter:description content="In my previous blog post, I discussed JAX - a framework for high performance numerical computing and machine learning - in an atypical manner. I didn&rsquo;t create a single training loop, and only showed a couple patterns that looked vaguely machine learning-like. If you haven&rsquo;t read that blog post yet, you can read it here.
This approach was deliberate as I felt that JAX - although designed for machine learning research - is more general-purpose than that."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://afmck.in/posts/"},{"@type":"ListItem","position":3,"name":"Easy JAX training loops with Flax and Optax","item":"https://afmck.in/posts/2023-06-04-flax-post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Easy JAX training loops with Flax and Optax","name":"Easy JAX training loops with Flax and Optax","description":"In my previous blog post, I discussed JAX - a framework for high performance numerical computing and machine learning - in an atypical manner. I didn\u0026rsquo;t create a single training loop, and only showed a couple patterns that looked vaguely machine learning-like. If you haven\u0026rsquo;t read that blog post yet, you can read it here.\nThis approach was deliberate as I felt that JAX - although designed for machine learning research - is more general-purpose than that.","keywords":[],"articleBody":"In my previous blog post, I discussed JAX - a framework for high performance numerical computing and machine learning - in an atypical manner. I didn’t create a single training loop, and only showed a couple patterns that looked vaguely machine learning-like. If you haven’t read that blog post yet, you can read it here.\nThis approach was deliberate as I felt that JAX - although designed for machine learning research - is more general-purpose than that. The steps to use it are to define what you want to happen, wrap it in within jax.jit, let JAX trace out your function into an intermediate graph representation, which is then passed to XLA where it will be compiled and optimised. The result is a single, heavily-optimised, binary blob, ready and waiting to receive your data. This approach is a natural fit for many machine learning applications, as well as other scientific computing tasks. Therefore, targeting machine learning only didn’t make sense. It is also ground that has already been extensively covered\nI wanted to do a different take on introductory JAX. In the previous post, I mentioned that it is possible to develop a full machine learning training loop - models, optimisers and all - in pure JAX. This is self-evident as JAX is general-purpose. It is a good exercise, but not a strategy I like to employ. In this blog post I want to introduce two higher level libraries built on top of JAX, that do a lot of the heavy lifting for us when writing machine learning applications. These libraries are Flax and Optax.\nTo summarise the libraries:\nJAX - provides a high-level neural network API that lets the developer reason about the model in terms of components, like in PyTorch, rather than with JAX functions that take parameters as inputs. Optax - a library containing a vast array of model training utilities, such as optimisers, loss functions, learning rate schedulers, and more! Very batteries-included. At the end of this post, we will have implemented and trained a very simple class-conditioned image generation model called a variational autoencoder (VAE) to generate MNIST digits.\nNeural Network API with Flax The high level structure of a training loop in pure JAX, looks something like this:\ndataset = ... # initialise training dataset that we can iterate over params = ... # initialise trainable parameters of our model epochs = ... def model_forward(params, batch): ... # perform a forward pass of our model on `batch` using `params` return outputs def loss_fn(params, batch): model_output = model_forward(params, batch) loss = ... # compute a loss based on `batch` and `model_output` return loss @jax.jit def train_step(params, batch): loss, grads = jax.value_and_grad(loss_fn)(params, batch) grads = ... # transform `grads` (clipping, multiply by learning rate, etc.) params = ... # update `params` using `grads` (such as via SGD) return params, loss for _ in range(epochs): for batch in dataset: params, loss = train_step(params, batch) ... # report metrics like loss, accuracy, and the like. We define our model in a functional manner: a function that takes the model parameters and a batch as input, and returns the output of the model. Similarly, we define the loss function that also takes the parameters and a batch as input, but returns the loss instead.\nOur final function is the train step itself which we wrap in jax.jit – giving XLA maximum context to compile and optimise the training step. This first computes the gradient of the loss function using the function transform jax.value_and_grad, manipulates the returned gradients (perhaps scaling by a learning rate), and updates the parameters. We return the new parameters, and use them on the next call to train_step. This is called in a loop, fetching new batches from the dataset before each training step.\nMost machine learning programs follow a pattern such as the one above. But in frameworks like PyTorch, we package together the model forward pass and the management of model parameters into a stateful object representing our model – simplifying the training loop. It would be nice if we could imitate this behaviour in stateless JAX to allow the developer to reason about models in a class-based way. This is what Flax’s neural network API – flax.linen – aims to achieve.\nWhether or not writing models in a purely stateless, functional way is better than a stateful, class-based way, is not the topic of this blog post. Both have merits. Regardless, during execution the final result is the same whether we use Flax or not. We get a stateless, heavily-optimised, binary blob that we throw data at. It’s all JAX after all.\nThere are two main ways to define a module in Flax: one is PyTorch-like and the other is a compact representation:\nimport flax.linen as nn from typing import Callable class Model(nn.Module): dim: int activation_fn: Callable = nn.relu def setup(self): self.layer = nn.Dense(self.dim) def __call__(self, x): x = self.layer(x) return self.activation_fn(x) class ModelCompact(nn.Module): dim: int activation_fn: Callable = nn.relu @nn.compact def __call__(self, x): x = nn.Dense(self.dim)(x) return self.activation_fn(x) If we have complex initialisation logic, the former may be more appropriate. If the module is relatively simple, we can make use of the nn.compact representation to automatically define the module by the forward pass alone.\nLike other frameworks, we can nest modules within each other to implement complex model behaviour. Like we’ve already seen, flax.linen provides some pre-baked modules like nn.Dense (same as PyTorch’s nn.Linear). I won’t enumerate them all, but the usual candidates are all there like convolutions, embeddings, and more.\nSomething to bear in mind if you are porting models from PyTorch to Flax is that the default weight initialisation may be different. For example, in PyTorch the default bias initialisation is the LeCun normal, but in Flax it is initialised to zero.\nHowever, currently we cannot call this model, even if we were to initialise the class itself. There simply aren’t any parameters to use. Furthermore, the module is never a container for parameters. An instance of a Flax module is simply a hollow shell, that loosely associates operations with parameters and inputs that are passed as input later.\nTo see what I mean, let’s initialise some parameters for our model:\nkey = jax.random.PRNGKey(0xffff) key, model_key = jax.random.split(key) model = Model(dim=4) params = model.init(model_key, jnp.zeros((1, 8))) params === Out: FrozenDict({ params: { layer: { kernel: Array([[-0.05412389, -0.28172645, -0.07438638, 0.5238516 ], [-0.13562573, -0.17592733, 0.45305118, -0.0650041 ], [ 0.25177842, 0.13981569, -0.41496065, -0.15681015], [ 0.13783392, -0.6254694 , -0.09966562, -0.04283331], [ 0.48194656, 0.07596914, 0.0429794 , -0.2127948 ], [-0.6694777 , 0.15849823, -0.4057232 , 0.26767966], [ 0.22948688, 0.00706845, 0.0145666 , -0.1280596 ], [ 0.62309605, 0.12575962, -0.05112049, -0.316764 ]], dtype=float32), bias: Array([0., 0., 0., 0.], dtype=float32), }, }, }) In the above cell, we first initialised our model class, which returns an instance of Model which we assign to the variable model. Like I said, it does not contain any parameters, it is just a hollow shell that we pass parameters and inputs to. We can see this by printing the model variable itself:\nmodel === Out: Model( # attributes dim = 4 activation_fn = relu ) We can also call the module itself, which will fail even though we have defined the __call__ method:\nmodel(jnp.zeros((1, 8))) === Out: /usr/local/lib/python3.10/dist-packages/flax/linen/module.py in __getattr__(self, name) 935 msg += (f' If \"{name}\" is defined in \\'.setup()\\', remember these fields ' 936 'are only accessible from inside \\'init\\' or \\'apply\\'.') --\u003e 937 raise AttributeError(msg) 938 939 def __dir__(self) -\u003e List[str]: AttributeError: \"Model\" object has no attribute \"layer\". If \"layer\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'. To initialise the parameters, we passed a PRNG key and some dummy inputs to the model’s init function of the same shape and dtype as the inputs we will use later. In this simple case, we just pass x as in the original module’s __call__ definition, but could be multiple arrays, PyTrees, or PRNG keys. We need the input shapes and dtypes in order to determine the shape and dtype of the model parameters.\nFrom the model.init call, we get a nested FrozenDict holding our model’s parameters. If you have seen PyTorch state dictionaries, the format of the parameters is similar: nested dictionaries with meaningful named keys, with parameter arrays as values. If you’ve read my previous blog post or read about JAX before, you will know that this structure is a PyTree. Not only does Flax help developers loosely associate parameters and operations, it also helps initialise model parameters based on the model definition.\nWith the parameters, we can call the model using model.apply – providing the parameters and inputs:\nkey, x_key = jax.random.split(key) x = jax.random.normal(x_key, (1, 8)) y = model.apply(params, x) y === Out: Array([[0.9296505 , 0.25998798, 0.01101626, 0. ]], dtype=float32) There is nothing special about the PyTree returned by model.init – it is just a regular PyTree storing the model’s parameters. params can be swapped with any other PyTree that contains the parameters model expects:\nzero_params = jax.tree_map(jnp.zeros_like, params) # generates a PyTree with same structure as `params` will all values set to 0. print(zero_params) model.apply(zero_params, x) === Out: FrozenDict({ params: { layer: { bias: Array([0., 0., 0., 0.], dtype=float32), kernel: Array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], dtype=float32), }, }, }) Array([[0., 0., 0., 0.]], dtype=float32) Forcing model calls to require explicitly passing parameters keeps it stateless and returning parameters like any other PyTree, makes Flax interoperable with JAX functions – as well as other libraries built on JAX. Essentially, by using Flax we aren’t forced to use any other specific frameworks and have access to all regular JAX features.\nIf you are used to frameworks like PyTorch, calling models like this feels unnatural at first. However, I personally quite like it this way – it feels rather elegant to pass different parameters to the model to get different behaviour rather than “load” the weights. A bit subjective and fuzzy, I know, but I like it.\nTo summarise the difference, if we aim to implement $f_\\theta(x)$, a PyTorch module is basically $f_\\theta$ (which we can call on $x$). A Flax module is simply $f$, which needs to be provided parameters $\\theta$ before it can be called on $x$ – or alternatively, we call $f$ on $(\\theta, x)$.\nAll in all, the point of Flax is to provide a familiar stateful API for development whilst preserving JAX statelessness during runtime. We can build our neural network modules in terms of classes and objects, but the final result is a stateless function model.apply that takes in our inputs and a PyTree of parameters.\nThis is identical behaviour to what we began with (recall our model_forward function at the start of this section), just now tied up nicely together. Therefore, our function containing model.apply that takes as input our PyTree, can be safely jit-compiled. The result is the same, a heavily-optimised binary blob we bombard with data. Nothing changes during runtime, it just makes development easier for those who prefer reasoning about neural networks in a class-based way whilst remaining interoperable with, and keeping the performance of JAX.\nThere’s a lot more to Flax than this, especially outside the flax.linen neural network API. For now though, we will move on to developing a full training loop using Flax and Optax. We will swing back around to some extra Flax points later, but I feel some concepts are hard to explain without first showing a training loop.\nA full training loop with Optax and Flax We’ve shown how to reduce the complexity of writing model code and parameter initialisation. We can push this further by relying on Optax to handle the gradient manipulation and parameter updates in train_step. For simple optimisers, these steps can be quite simple. However, for more complex optimisers or gradient transformation behaviour, it can get quite complex to implement in JAX alone. Optax packages this complex behaviour into a simple API.\nimport optax optimiser = optax.sgd(learning_rate=1e-3) optimiser === Out: GradientTransformationExtraArgs(init=\u003cfunction chain.\u003clocals\u003e.init_fn at 0x7fa7185503a0\u003e, update=\u003cfunction chain.\u003clocals\u003e.update_fn at 0x7fa718550550\u003e) Not pretty, but we can see that the optimiser is just a gradient transformation – in fact all optimisers in Optax are implemented as gradient transformations. A gradient transformation is defined to be a pair of functions init and update, which are both pure functions. Like a Flax model, Optax optimisers have no state kept internally, and must be initialised before it can be used, and any state must be passed by the developer to update:\noptimiser_state = optimiser.init(params) optimiser_state === Out: (EmptyState(), EmptyState()) Of course, as SGD is a stateless optimiser, the initialisation call simply returns an empty state. It must return this to maintain the API of a gradient transformation. Let’s try with a more complex optimiser like Adam:\noptimiser = optax.adam(learning_rate=1e-3) optimiser_state = optimiser.init(params) optimiser_state === Out: (ScaleByAdamState(count=Array(0, dtype=int32), mu=FrozenDict({ params: { layer: { bias: Array([0., 0., 0., 0.], dtype=float32), kernel: Array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], dtype=float32), }, }, }), nu=FrozenDict({ params: { layer: { bias: Array([0., 0., 0., 0.], dtype=float32), kernel: Array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], dtype=float32), }, }, })), EmptyState()) Here, we can see the first and second order statistics of the Adam optimiser, as well as a count storing number of optimiser updates. Like with SGD, this state needs to be passed to update when called.\nLike Flax parameters, the optimiser state is just a PyTree. Any PyTree with a compatible structure could also be used. Again, this also allows interoperability with JAX and jax.jit, as well as other libraries built on top of JAX.\nConcretely, Optax gradient transformations are simply a named tuple containing pure functions init and update. init is a pure function which takes in an example instance of gradients to be transformed and returns the optimiser initial state. In the case of optax.sgd this returns an empty state regardless of the example provided. For optax.adam, we get a more complex state containing the first and second order statistics of the same PyTree structure as the provided example.\nupdate takes in a PyTree of updates with the same structure as the example instance provided to init. In addition, it takes in the optimiser state returned by init and optionally the parameters of the model itself, which may be needed for some optimisers. This function will return the transformed gradients (which could be another set of gradients, or the actual parameter updates) and the new optimiser state.\nThis is explained quite nicely in the documentation here\nIn action on some dummy data, we get the following:\nimport optax params = jnp.array([0.0, 1.0, 2.0]) # some dummy parameters optimiser = optax.adam(learning_rate=0.01) opt_state = optimiser.init(params) grads = jnp.array([4.0, 0.6, -3])# some dummy gradients updates, opt_state = optimiser.update(grads, opt_state, params) updates === Out: Array([-0.00999993, -0.00999993, 0.00999993], dtype=float32) Optax provides a helper function to actually apply the updates to our parameters:\nnew_params = optax.apply_updates(params, updates) new_params === Out: Array([-0.00999993, 0.99000007, 2.01 ], dtype=float32) It is important to emphasise that Optax optimisers are gradient transformations, but gradient transformations are not just optimisers. We’ll see more of that later after we finish the training loop.\nOn that note, let’s begin with said training loop. Recall that our goal is to train a class-conditioned, variational autoencoder (VAE) on the MNIST dataset.\nI chose this example as it is slightly more interesting than the typical classification example found in most tutorials.\nNot strictly related to JAX, Flax, or Optax, but it is worth describing what a VAE is. First, an autoencoder model is one that maps some input $x$ in our data space to a latent vector $z$ in the latent space (a space with smaller dimensionality than the data space) and back to the data space. It is trained to minimise the reconstruction loss between the input and the output, essentially learning the identity function through an information bottleneck.\nThe portion of the network that maps from the data space to the latent space is called the encoder and the portion that maps from the latent space to the data space is called the decoder. Applying the encoder is somewhat analogous to lossy compression. Likewise, *applying the decoder is akin to lossy decompression.\nWhat makes a VAE different to an autoencoder is that the encoder does not output the latent vector directly. Instead, it outputs the mean and log-variance of a Gaussian distribution, which we then sample from in order to obtain our latent vector. We apply an extra loss term to make these mean and log-variance outputs roughly follow the standard normal distribution.\nInterestingly, defining the encoder this way means for every given input $x$ we have many possible latent vectors which are sampled stochastically. Our encoder is almost mapping to a sphere of possible latents centred at the mean vector with radius scaling with log-variance.\nThe decoder is the same as before. However, now we can sample a latent from the normal distribution and pass it to the decoder in order to generate samples like those in the dataset! Adding the variational component turns our autoencoder compression model into a VAE generative model.\nOur goal is to implement the model code for the VAE as well as the training loop with both the reconstruction and variational loss terms. Then, we can sample new digits that look like those in the MNIST dataset! Additionally, we will provide an extra input to the model – the class index – so we can control which number we want to generate.\nLet’s begin by defining our configuration. For this educational example, we will just define some constants in a cell:\nbatch_size = 16 latent_dim = 32 kl_weight = 0.5 num_classes = 10 seed = 0xffff Along with some imports and PRNG initialisation:\nimport jax # install correct wheel for accelerator you want to use import flax import optax import orbax import flax.linen as nn import jax.numpy as jnp import numpy as np from jax.typing import ArrayLike from typing import Tuple, Callable from math import sqrt import torchvision.transforms as T from torchvision.datasets import MNIST from torch.utils.data import DataLoader key = jax.random.PRNGKey(seed) Let’s grab our MNIST dataset while we are here:\ntrain_dataset = MNIST('data', train = True, transform=T.ToTensor(), download=True) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) JAX, Flax, and Optax do not have data loading utilities, so I just use the perfectly serviceable PyTorch implementation of the MNIST dataset here.\nNow to our first real Flax model. We begin by defining a submodule FeedForward that implements a stack of linear layers with intermediate non-linearities:\nclass FeedForward(nn.Module): dimensions: Tuple[int] = (256, 128, 64) activation_fn: Callable = nn.relu drop_last_activation: bool = False @nn.compact def __call__(self, x: ArrayLike) -\u003e ArrayLike: for i, d in enumerate(self.dimensions): x = nn.Dense(d)(x) if i != len(self.dimensions) - 1 or not self.drop_last_activation: x = self.activation_fn(x) return x key, model_key = jax.random.split(key) model = FeedForward(dimensions = (4, 2, 1), drop_last_activation = True) print(model) params = model.init(model_key, jnp.zeros((1, 8))) print(params) key, x_key = jax.random.split(key) x = jax.random.normal(x_key, (1, 8)) y = model.apply(params, x) y === Out: FeedForward( # attributes dimensions = (4, 2, 1) activation_fn = relu drop_last_activation = True ) FrozenDict({ params: { Dense_0: { kernel: Array([[ 0.0840368 , -0.18825287, 0.49946404, -0.4610112 ], [ 0.4370267 , 0.21035315, -0.19604324, 0.39427406], [ 0.00632685, -0.02732705, 0.16799504, -0.44181877], [ 0.26044282, 0.42476758, -0.14758752, -0.29886967], [-0.57811564, -0.18126923, -0.19411889, -0.10860331], [-0.20605426, -0.16065307, -0.3016759 , 0.44704655], [ 0.35531637, -0.14256613, 0.13841921, 0.11269159], [-0.430825 , -0.0171169 , -0.52949774, 0.4862139 ]], dtype=float32), bias: Array([0., 0., 0., 0.], dtype=float32), }, Dense_1: { kernel: Array([[ 0.03389561, -0.00805947], [ 0.47362345, 0.37944487], [ 0.41766328, -0.15580587], [ 0.5538078 , 0.18003668]], dtype=float32), bias: Array([0., 0.], dtype=float32), }, Dense_2: { kernel: Array([[ 1.175035 ], [-1.1607001]], dtype=float32), bias: Array([0.], dtype=float32), }, }, }) Array([[0.5336972]], dtype=float32) We use the nn.compact decorator here as the logic is relatively simple. We iterate over the tuple self.dimensions and pass our current activations through a nn.Dense module, followed by applying self.activation_fn. This activation can optionally be dropped for the final linear layer in FeedForward. This is needed as nn.relu only outputs non-negative values, whereas sometimes we need non-negative outputs!\nUsing FeedForward, we can define our full VAE model:\nclass VAE(nn.Module): encoder_dimensions: Tuple[int] = (256, 128, 64) decoder_dimensions: Tuple[int] = (128, 256, 784) latent_dim: int = 4 activation_fn: Callable = nn.relu def setup(self): self.encoder = FeedForward(self.encoder_dimensions, self.activation_fn) self.pre_latent_proj = nn.Dense(self.latent_dim * 2) self.post_latent_proj = nn.Dense(self.encoder_dimensions[-1]) self.class_proj = nn.Dense(self.encoder_dimensions[-1]) self.decoder = FeedForward(self.decoder_dimensions, self.activation_fn, drop_last_activation=False) def reparam(self, mean: ArrayLike, logvar: ArrayLike, key: jax.random.PRNGKey) -\u003e ArrayLike: std = jnp.exp(logvar * 0.5) eps = jax.random.normal(key, mean.shape) return eps * std + mean def encode(self, x: ArrayLike): x = self.encoder(x) mean, logvar = jnp.split(self.pre_latent_proj(x), 2, axis=-1) return mean, logvar def decode(self, x: ArrayLike, c: ArrayLike): x = self.post_latent_proj(x) x = x + self.class_proj(c) x = self.decoder(x) return x def __call__( self, x: ArrayLike, c: ArrayLike, key: jax.random.PRNGKey) -\u003e Tuple[ArrayLike, ArrayLike, ArrayLike]: mean, logvar = self.encode(x) z = self.reparam(mean, logvar, key) y = self.decode(z, c) return y, mean, logvar key = jax.random.PRNGKey(0x1234) key, model_key = jax.random.split(key) model = VAE(latent_dim=4) print(model) key, call_key = jax.random.split(key) params = model.init(model_key, jnp.zeros((batch_size, 784)), jnp.zeros((batch_size, num_classes)), call_key) recon, mean, logvar = model.apply(params, jnp.zeros((batch_size, 784)), jnp.zeros((batch_size, num_classes)), call_key) recon.shape, mean.shape, logvar.shape === Out: ClassVAE( # attributes encoder_dimensions = (256, 128, 64) decoder_dimensions = (128, 256, 784) latent_dim = 4 activation_fn = relu ) ((16, 784), (16, 4), (16, 4)) There is a lot to the above cell. Knowing the specifics of how this model works isn’t too important to understanding the training loop later, as we can treat the model as a bit of a black box. Simply substitute your own model of choice. Saying that, I’ll unpack each function briefly:\nsetup: Creates the submodules of the network, namely two FeedForward stacks and two nn.Linear layers that project to and from the latent space. Additionally, it initialises a third nn.Linear layer that projects our class conditioning vector to the same dimensionality as the last encoder layer. reparam: Sampling a latent directly from a random Gaussian is not differentiable, hence we employ the reparameterisation trick. This involves sampling a random vector, scaling by the standard deviation, then adding to the mean. As it involves random array generation, we take as input a key in addition to the mean and log-variance. encode: Applies the encoder and projection to the latent space to the input. Note, the output of the projection is actually double the size of the latent space, as we split it in twine to obtain our mean and log-variance. decode: Applies a projection from the latent space to x, followed by adding the output of class_proj on the conditioning vector. Finally, it passes the result through the decoder stack. __call__: This is simply the full model forward pass: encode then reparam then decode. This is used during training. The above example also demonstrates that we can add other functions to our Flax modules aside from setup and __call__. This is useful for more complex behaviour, or if we want to only execute parts of the model (more on this later).\nWe now have our model, optimiser, and dataset. The next step is to write the function that implements our training step and then jit-compile it:\ndef create_train_step(key, model, optimiser): params = model.init(key, jnp.zeros((batch_size, 784)), jnp.zeros((batch_size, num_classes)), jax.random.PRNGKey(0)) # dummy key just as example input opt_state = optimiser.init(params) def loss_fn(params, x, c, key): reduce_dims = list(range(1, len(x.shape))) c = jax.nn.one_hot(c, num_classes) # one hot encode the class index recon, mean, logvar = model.apply(params, x, c, key) mse_loss = optax.l2_loss(recon, x).sum(axis=reduce_dims).mean() kl_loss = jnp.mean(-0.5 * jnp.sum(1 + logvar - mean ** 2 - jnp.exp(logvar), axis=reduce_dims)) # KL loss term to keep encoder output close to standard normal distribution. loss = mse_loss + kl_weight * kl_loss return loss, (mse_loss, kl_loss) @jax.jit def train_step(params, opt_state, x, c, key): losses, grads = jax.value_and_grad(loss_fn, has_aux=True)(params, x, c, key) loss, (mse_loss, kl_loss) = losses updates, opt_state = optimiser.update(grads, opt_state, params) params = optax.apply_updates(params, updates) return params, opt_state, loss, mse_loss, kl_loss return train_step, params, opt_state Here, I don’t define the training step directly, but rather first define a function that returns the training step function given a target model and optimiser, along with returning the freshly initialised parameters and optimiser state.\nLet us unpack it all:\nFirst, it initialises our model using an example input. In this case, this is a 784-dim array which contains the (flattened) MNIST digit and a random, random key. Also initialises the optimiser state using the parameters we just initialised. Now, it defines the loss function. This is simply a model.apply call which returns the model’s reconstruction of the input, along with the predicted mean and log-variance. We then compute the mean-squared error loss and the KL-divergence, before finally computing a weighted sum to get our final loss. The KL loss term is what keeps the encoder outputs close to a standard normal distribution. Next, the actual train step definition. This begins by transforming loss_fn using our old friend jax.value_and_grad which will return the loss and also the gradients. We must set has_aux=True as we return all individual loss terms for logging purposes. We provide the gradients, optimiser state, and parameters to optimiser.update which returns the transformed gradients and the new optimiser state. The transformed gradients are then applied to the parameters. Finally, we return the new parameters, optimiser state, and loss terms – followed by wrapping the whole thing in jax.jit. Phew.. A function that generates the training step is just a pattern I quite like, and there is nothing stopping you from just writing the training step directly.\nLet’s call create_train_step:\nkey, model_key = jax.random.split(key) model = VAE(latent_dim=latent_dim) optimiser = optax.adamw(learning_rate=1e-4) train_step, params, opt_state = create_train_step(model_key, model, optimiser) When we call the above, we get a train_step ready to be compiled and accept our parameters, optimiser state, and data at blistering fast speeds. As always with jit-compiled functions, the first call with a given set of input shapes will be slow, but fast on subsequent calls as we skip the compiling and optimisation process.\nWe are now in a position to write our training loop and train the model!\nfreq = 100 for epoch in range(10): total_loss, total_mse, total_kl = 0.0, 0.0, 0.0 for i, (batch, c) in enumerate(train_loader): key, subkey = jax.random.split(key) batch = batch.numpy().reshape(batch_size, 784) c = c.numpy() params, opt_state, loss, mse_loss, kl_loss = train_step(params, opt_state, batch, c, subkey) total_loss += loss total_mse += mse_loss total_kl += kl_loss if i \u003e 0 and not i % freq: print(f\"epoch {epoch} | step {i} | loss: {total_loss / freq} ~ mse: {total_mse / freq}. kl: {total_kl / freq}\") total_loss = 0. total_mse, total_kl = 0.0, 0.0 === Out: epoch 0 | step 100 | loss: 49.439998626708984 ~ mse: 49.060447692871094. kl: 0.7591156363487244 epoch 0 | step 200 | loss: 37.1823616027832 ~ mse: 36.82903289794922. kl: 0.7066375613212585 epoch 0 | step 300 | loss: 33.82365036010742 ~ mse: 33.49456024169922. kl: 0.6581906080245972 epoch 0 | step 400 | loss: 31.904821395874023 ~ mse: 31.570871353149414. kl: 0.6679074764251709 epoch 0 | step 500 | loss: 31.095705032348633 ~ mse: 30.763246536254883. kl: 0.6649144887924194 epoch 0 | step 600 | loss: 29.771989822387695 ~ mse: 29.42426872253418. kl: 0.6954278349876404 ... epoch 9 | step 3100 | loss: 14.035745620727539 ~ mse: 10.833460807800293. kl: 6.404574871063232 epoch 9 | step 3200 | loss: 14.31241226196289 ~ mse: 11.043667793273926. kl: 6.53748893737793 epoch 9 | step 3300 | loss: 14.26440143585205 ~ mse: 11.01070785522461. kl: 6.5073771476745605 epoch 9 | step 3400 | loss: 13.96005630493164 ~ mse: 10.816412925720215. kl: 6.28728723526001 epoch 9 | step 3500 | loss: 14.166285514831543 ~ mse: 10.919700622558594. kl: 6.493169784545898 epoch 9 | step 3600 | loss: 13.819541931152344 ~ mse: 10.632755279541016. kl: 6.373570919036865 epoch 9 | step 3700 | loss: 14.452215194702148 ~ mse: 11.186063766479492. kl: 6.532294750213623 Now that we have our train_step function, the training loop itself is just repeatedly fetching data, calling our uber-fast train_step function, and logging results so we can track training. We can see that the loss is decreasing, which means our model is training!\nNote that the KL-loss term increases during training. This is okay so long as it doesn’t get too high, in which case sampling from the model becomes impossible. Tuning the hyperparameter kl_weight is quite important. Too low and we get perfect reconstructions but no sampling capabilities – too high and the outputs will become blurry.\nLet’s sample from the model so we can see that it does indeed produce some reasonable samples:\ndef build_sample_fn(model, params): @jax.jit def sample_fn(z: jnp.array, c: jnp.array) -\u003e jnp.array: return model.apply(params, z, c, method=model.decode) return sample_fn sample_fn = build_sample_fn(model, params) num_samples = 100 h, w = 10 key, z_key = jax.random.split(key) z = jax.random.normal(z_key, (num_samples, latent_dim)) c = np.repeat(np.arange(h)[:, np.newaxis], w, axis=-1).flatten() c = jax.nn.one_hot(c, num_classes) sample = sample_fn(z, c) z.shape, c.shape, sample.shape === Out: ((100, 32), (100, 10), (100, 784)) The above cell generates 100 samples – 10 examples from each of the 10 classes. We jit-compile our sample function in case we want to sample again later. We only call the model.decode method, rather than the full model, as we only need to decode our randomly sampled latents. This is achieved by specifying method=model.decode in the model.apply call.\nLet’s visualise the results using matplotlib:\nimport matplotlib.pyplot as plt import math from numpy import einsum sample = einsum('ikjl', np.asarray(sample).reshape(h, w, 28, 28)).reshape(28*h, 28*w) plt.imshow(sample, cmap='gray') plt.show() It seems our model did indeed train and can be sampled from! Additionally, the model is capable of using the class conditioning signal so that we can control which digits are generated. Therefore, we have succeeded in building a full training loop using Flax and Optax!\nExtra Flax and Optax Tidbits I’d like to finish this blog post by highlighting some interesting and useful features that may prove useful in your own applications. I won’t delve into great detail but simply summarise and point you in the right direction.\nYou may have noticed already that when we add parameters, optimiser states, and a bunch of other metrics to the return call of train_step it gets a bit unwieldy to handle all the state. It could get worse if we later need a more complex state. One solution would be to return a namedtuple so we can at least package the state together somewhat. However, Flax provides its own solution, flax.training.train_state.TrainState, which has some extra functions that make updating the combined state (model and optimiser state) easier.\nIt is easiest to show by simply taking our earlier train_step and refactoring it with TrainState:\nfrom flax.training.train_state import TrainState def create_train_step(key, model, optimiser): params = model.init(key, jnp.zeros((batch_size, 784)), jnp.zeros((batch_size, num_classes)), jax.random.PRNGKey(0)) state = TrainState.create(apply_fn=model.apply, params=params, tx=optimiser) def loss_fn(state, x, c, key): reduce_dims = list(range(1, len(x.shape))) c = jax.nn.one_hot(c, num_classes) recon, mean, logvar = state.apply_fn(state.params, x, c, key) mse_loss = optax.l2_loss(recon, x).sum(axis=reduce_dims).mean() kl_loss = jnp.mean(-0.5 * jnp.sum(1 + logvar - mean ** 2 - jnp.exp(logvar), axis=reduce_dims)) loss = mse_loss + kl_weight * kl_loss return loss, (mse_loss, kl_loss) @jax.jit def train_step(state, x, c, key): losses, grads = jax.value_and_grad(loss_fn, has_aux=True)(state, x, c, key) loss, (mse_loss, kl_loss) = losses state = state.apply_gradients(grads=grads) return state, loss, mse_loss, kl_loss return train_step, state We begin create_train_step by initialising our parameters as before. However, the next step is now to create the state using TrainState.create and passing our model forward call, the initialised parameters, and the optimiser we want to use. Internally, TrainState.create will initialise and store the optimiser state for us.\nIn loss_fn, rather than call model.apply we can use state.apply_fn instead. Either method is equivalent, just that sometimes we may not have model in scope and so can’t access model.apply.\nThe largest change is in train_step itself. Rather than call optimiser.update followed by optax.apply_updates, we simply call state.apply_gradients which internally updates the optimiser state and the parameters. It then returns the new state, which we return and pass to the next call of train_step – as we would with params and opt_state.\nIt is possible to add extra attributes to TrainState by subclassing it, for example adding attributes to store the latest loss.\nIn conclusion, TrainState makes it easier to pass around state in the training loop, as well as abstracting away optimiser and parameter updates.\nAnother useful feature of Flax is the ability to bind parameters to a model, yielding an interactive instance that can be called directly, as if it were a PyTorch model with internal state. However, this state is static and can only change if we bind it again, which makes it unusable for training. However, it can be handy for interactive debugging or inference.\nThe API is pretty simple:\nkey, model_key = jax.random.split(key) model = nn.Dense(2) params = model.init(model_key, jnp.zeros(8)) bound_model = model.bind(params) bound_model(jnp.ones(8)) === Out: Array([ 0.45935923, -0.691003 ], dtype=float32) We can get back the unbound model and its parameters by calling model.unbind:\nbound_model.unbind() === Out: (Dense( # attributes features = 2 use_bias = True dtype = None param_dtype = float32 precision = None kernel_init = init bias_init = zeros dot_general = dot_general ), FrozenDict({ params: { kernel: Array([[-0.11450272, -0.2808447 ], [-0.45104247, -0.3774913 ], [ 0.07462895, 0.3622056 ], [ 0.59189916, -0.34050766], [-0.10401642, -0.36226135], [ 0.157985 , 0.00198693], [-0.00792678, -0.1142673 ], [ 0.31233454, 0.4201768 ]], dtype=float32), bias: Array([0., 0.], dtype=float32), }, })) I said I wouldn’t enumerate layers in Flax as I don’t see much value in doing so, but I will highlight two particularly interesting ones. First is nn.Dropout which is numerically the same as its PyTorch counterpart, but like anything random in JAX, requires a PRNG key as input.\nThe dropout layer takes its random key by internally calling self.make_rng('dropout'), which pulls and splits from a PRNG stream named 'dropout'. This means when we call model.apply we will need to define the starting key for this PRNG stream. This can be done by passing a dictionary mapping stream names to PRNG keys, to the rngs argument in model.apply:\nkey, x_key = jax.random.split(key) key, drop_key = jax.random.split(key) x = jax.random.normal(x_key, (3,3)) model = nn.Dropout(0.5, deterministic=False) y = model.apply({}, x, rngs={'dropout': drop_key}) # there is no state, just pass empty dictionary :) x, y === Out: (Array([[ 1.7353934, -1.741734 , -1.3312583], [-1.615281 , -0.6381292, 1.3057163], [ 1.2640097, -1.986926 , 1.7818599]], dtype=float32), Array([[ 3.4707868, 0. , -2.6625166], [ 0. , 0. , 2.6114326], [ 0. , -3.973852 , 0. ]], dtype=float32)) model.init also accepts a dictionary of PRNG keys. If you pass in a single key like we have done so far, it starts a stream named 'params'. This is equivalent to passing {'params': rng} instead.\nThe streams are accessible to submodules, so nn.Dropout can call self.make_rng('dropout') regardless of where it is in the model. We can define our own PRNG streams by specifying them in the model.apply call. In our VAE example, we could forgo passing in the key manually, and instead get keys for random sampling using self.make_rng('noise') or similar, then passing a starting key in rngs in model.apply. For models with lots of randomness, it may be worth doing this.\nThe second useful built-in module is nn.Sequential which is again like its PyTorch counterpart. This simply chains together many modules such that the outputs of one module will flow into the inputs of the next. Useful if we want to define large stacks of layers quickly.\nNow onto some Optax tidbits! First, Optax comes with a bunch of learning rate schedulers. Instead of passing a float value to learning_rate when creating the optimiser, we can pass a scheduler. When applying updates, Optax will automatically select the correct learning rate. Let’s define a simple, linear schedule:\nstart_lr, end_lr = 1e-3, 1e-5 steps = 10_000 lr_scheduler = optax.linear_schedule( init_value=start_lr, end_value=end_lr, transition_steps=steps, ) optimiser = optax.adam(learning_rate=lr_scheduler) You can join together schedulers using optax.join_schedules in order to get more complex behaviour like learning rate warmup followed by decay:\nwarmup_start_lr, warmup_steps = 1e-6, 1000 start_lr, end_lr, steps = 1e-2, 1e-5, 10_000 lr_scheduler = optax.join_schedules( [ optax.linear_schedule( warmup_start_lr, start_lr, warmup_steps, ), optax.linear_schedule( start_lr, end_lr, steps - warmup_steps, ), ], [warmup_steps], ) optimiser = optax.adam(lr_scheduler) The last argument to optax.join_schedules should be a sequence of integers defining the step boundaries between different schedules. In this case, we switch from warmup to decay after warmup_steps steps.\nOptax keeps track of the number of optimiser steps in its opt_state, so we don’t need to track this ourselves.\nSimilar to joining schedulers, Optax supports chaining optimisers together. More specifically, the chaining of gradient transformations:\noptimiser = optax.chain( optax.clip_by_global_norm(1.0), optax.adam(1e-2), ) When calling optimiser.update, the gradients will first be clipped before then doing the regular Adam update. Chaining together transformations like this is quite an elegant API and allows for complex behaviour. To illustrate, adding exponential moving averages (EMA) of our updates in something like PyTorch is non-trivial, whereas in Optax it is as simple as adding optax.ema to our optax.chain call:\noptimiser = optax.chain( optax.clip_by_global_norm(1.0), optax.adam(1e-2), optax.ema(decay=0.999) ) In this case, optax.ema is a transformation on the final updates, rather than on the unprocessed gradients.\nGradient accumulation is implemented in Optax as a optimiser wrapper, rather than as a gradient transformation:\ngrad_accum = 4 optimiser = optax.MultiSteps(optax.adam(1e-2), grad_accum) The returned optimiser collects updates over the optimiser.update calls until grad_accum steps have occurred. In the intermediate steps, the returned updates will be a PyTree of zeros in the same shape as params, resulting in no update. Every grad_accum steps, the accumulated updates will be returned.\ngrad_accum can also be a function, which gives us a way to vary the batch size during training via adjusting the number of steps between parameter updates.\nHow about if we only want to train certain parameters? For example, when finetuning a pretrained model. Nowadays, this is a pretty common thing to do, taking pretrained large language models and adapting them for specific downstream tasks.\nLet’s grab a pretrained BERT model from the Huggingface hub:\nfrom transformers import FlaxBertForSequenceClassification model = FlaxBertForSequenceClassification.from_pretrained('bert-base-uncased') model.params.keys() === Out: dict_keys(['bert', 'classifier']) Huggingface provides Flax versions of most of their models. The API to use them is a bit different, calling model(**inputs, params=params) rather than model.apply. Providing no parameters will use the pretrained weights stored in model.params which is useful for inference-only tasks, but for training we need to pass the current parameters to the call.\nWe can see there are two top-level keys in the parameter PyTree: bert and classifier. Suppose we only want to finetune the classifier head and leave the BERT backbone alone, we can achieve this using optax.multi_transform:\noptimiser = optax.multi_transform({'train': optax.adam(1e-3), 'freeze': optax.set_to_zero()}, {'bert': 'freeze', 'classifier': 'train'}) opt_state = optimiser.init(model.params) grads = jax.tree_map(jnp.ones_like, model.params) updates, opt_state = optimiser.update(grads, opt_state, model.params) optax.multi_transform takes two inputs, the first is mapping from labels to gradient transformations. The second is a PyTree with the same structure or prefix as the updates (in the case above we use the prefix approach) mapping to labels. The transformation matching the label of a given update will be applied. This allows the partitioning of parameters and applying different updates to different parts.\nThe second argument can also be a function that, given the updates PyTree, returns such a PyTree mapping updates (or their prefix) to labels.\nThis can be used for other cases like having different optimisers for different layers (such as disabling weight decay for certain layers), but in our case we simply use optax.adam for our trainable parameters, and zero out gradients for other regions using the stateless transform optax.set_to_zero.\nIn jit-compiled function, the gradients that have optax.set_to_zero applied to them won’t be computed due to the optimisation process seeing that they will always be zero. Hence, we get the expected memory savings from only finetuning a subset of layers!\nLet’s print the updates so that we can see that we do indeed have no updates in the BERT backbone, and have updates in the classifier head:\nupdates['classifier'], updates['bert']['embeddings']['token_type_embeddings'] === Out: {'bias': Array([-0.00100002, -0.00100002], dtype=float32), 'kernel': Array([[-0.00100002, -0.00100002], [-0.00100002, -0.00100002], [-0.00100002, -0.00100002], ..., [-0.00100002, -0.00100002], [-0.00100002, -0.00100002], [-0.00100002, -0.00100002]], dtype=float32)} {'embedding': Array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)} We can verify that all updates are zero using jax.tree_util.tree_reduce:\njax.tree_util.tree_reduce(lambda c, p: c and (jnp.count_nonzero(p) == 0), updates['bert'], True) === Out: Array(True, dtype=bool) Both Flax and Optax are quite feature-rich despite the relative infancy of the JAX ecosystem. I’d recommend just opening the Flax or Optax API reference and searching for layers, optimisers, losses, and features you are used to having in other frameworks.\nThe last thing I want to talk about involves an entirely different library build on JAX. Orbax provides PyTree checkpointing utilities for saving and restoring arbitrary PyTrees. I won’t go into great detail but will show basic usage here. There is nothing worse than spending hours training only to forget about actually saving progress!\nHere is basic usage saving the BERT classifier parameters:\nimport orbax import orbax.checkpoint from flax.training import orbax_utils orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer() save_args = orbax_utils.save_args_from_target(model.params['classifier']) orbax_checkpointer.save('classifier.ckpt', model.params['classifier'], save_args=save_args) !ls === Out: classifier.ckpt Which we can restore by executing:\norbax_checkpointer.restore('classifier.ckpt') === Out: {'bias': array([0., 0.], dtype=float32), 'kernel': array([[-0.06871808, -0.06338844], [-0.03397266, 0.00899913], [-0.00669084, -0.06431466], ..., [-0.02699363, -0.03812294], [-0.00148801, 0.01149782], [-0.01051403, -0.00801195]], dtype=float32)} Which returns the raw PyTree. If you are using a custom dataclass with objects that can’t be serialised (such as a Flax train state where apply_fn and tx can’t be serialised) you can pass an example PyTree to item in the restore call, to let Orbax know the structure you want.\nManually saving checkpoints like this is a bit old-fashioned. Orbax has a bunch of automatic versioning and scheduling features built in, such as automatic deleting of old checkpoints, tracking the best metric, and more. To use these features, wrap the orbax_checkpointer in orbax.checkpoint.CheckpointManager:\noptions = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=4, create=True) checkpoint_manager = orbax.checkpoint.CheckpointManager( 'managed-checkpoint', orbax_checkpointer, options) for step in range(10): checkpoint_manager.save(step, model.params['classifier'], save_kwargs={'save_args': save_args}) !ls -l managed-checkpoint/* === Out: managed-checkpoint/6: total 4 drwxr-xr-x 2 root root 4096 Jun 3 09:07 default managed-checkpoint/7: total 4 drwxr-xr-x 2 root root 4096 Jun 3 09:07 default managed-checkpoint/8: total 4 drwxr-xr-x 2 root root 4096 Jun 3 09:07 default managed-checkpoint/9: total 4 drwxr-xr-x 2 root root 4096 Jun 3 09:07 default As we set max_to_keep=4, only the last four checkpoints have been kept.\nWe can view which steps have checkpoints:\ncheckpoint_manager.all_steps() === Out: [6, 7, 8, 9] As well as view if there is a checkpoint for a specific step:\ncheckpoint_manager.should_save(6) === Out: False And what the latest saved step was:\ncheckpoint_manager.latest_step() === Out: 9 We can restore using the checkpoint manager. Rather than provide a path to the restore function, we provide the step we want to restore:\nstep = checkpoint_manager.latest_step() checkpoint_manager.restore(step) === Out: {'bias': array([0., 0.], dtype=float32), 'kernel': array([[-0.06871808, -0.06338844], [-0.03397266, 0.00899913], [-0.00669084, -0.06431466], ..., [-0.02699363, -0.03812294], [-0.00148801, 0.01149782], [-0.01051403, -0.00801195]], dtype=float32)} For especially large checkpoints, Orbax supports asynchronous checkpointing which moves checkpointing to a background thread. You can do this by wrapping orbax.checkpoint.AsyncCheckpointer around the orbax.checkpoint.PyTreeCheckpointer we created earlier.\nYou may see mention online to Flax checkpointing utilities. However, these utilities are being deprecated and it is recommended to start using Orbax instead.\nThe documentation for Orbax is a bit spartan, but it has a fair few options to choose. It is worth just reading the CheckpointManagerOptions class here and seeing the available features.\nConclusion In this blog post, I’ve introduced two libraries built on top of JAX: Flax and Optax. This has been more of a practical guide into how you can implement training loops easily in JAX using these libraries, rather than a ideological discussion like my previous blog post on JAX.\nTo summarise this post:\nFlax provides a neural network API that allows the developer to build neural network modules in a class-based way. Unlike other frameworks, these modules do not contain state within them, essentially hollow shells that loosely associate functions with parameters and inputs, and provide easy methods to initialise the parameters. Optax provides a large suite of optimisers for updating our parameters. These, like Flax modules, do not contain state and must have state passed manually to it. All optimisers are simply gradient transformations: a pair of pure functions init and update. Optax also provides other gradient transformations and wrappers to allow for more complex behaviour, such as gradient clipping and parameter freezing. Both libraries simply operate on and return PyTrees and can easily interoperate with base JAX - crucially with jax.jit. This also makes them interoperable with other libraries based on JAX. For example, by choosing Flax, we aren’t locked into using Optax, and vice versa. There is a lot more to these two libraries than described here, but I hope this is a good starting point and can enable you to create your own training loops in JAX. A good exercise now would be to use the training loop and model code in this blog post and adapting it for your own tasks, such as another generative model.\nIf you liked this post please consider following me on Twitter or use this site’s RSS feed for notifications on future ramblings about machine learning and other topics. Alternatively you can navigate to the root of this website and repeatedly refresh until something happens. Thank you for reading this far and I hope you found it useful!\nAcknowledgements and Extra Resources Some good extra resources:\nMy previous blog post on JAX Aleksa Gordic’s JAX and Flax tutorial series Flax documentation Optax documentation Equinox - another neural network library built on JAX by Patrick Kidger Haiku - another neural network library built on JAX from Deepmind Orbax source code Found something wrong with this blog post? Let me know via email or Twitter!\n","wordCount":"7599","inLanguage":"en","datePublished":"2023-06-04T12:00:00+01:00","dateModified":"2023-06-04T12:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://afmck.in/posts/2023-06-04-flax-post/"},"publisher":{"@type":"Organization","name":"Alex McKinney","logo":{"@type":"ImageObject","url":"https://afmck.in/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://afmck.in accesskey=h title="Alex McKinney (Alt + H)">Alex McKinney</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://afmck.in/about title=About><span>About</span></a></li><li><a href=https://afmck.in/posts title=Posts><span>Posts</span></a></li><li><a href=https://afmck.in/cv.pdf title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Easy JAX training loops with Flax and Optax</h1><div class=post-meta><span title='2023-06-04 12:00:00 +0100 +0100'>June 4, 2023</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#neural-network-api-with-flax aria-label="Neural Network API with Flax">Neural Network API with Flax</a></li><li><a href=#a-full-training-loop-with-optax-and-flax aria-label="A full training loop with Optax and Flax">A full training loop with Optax and Flax</a></li><li><a href=#extra-flax-and-optax-tidbits aria-label="Extra Flax and Optax Tidbits">Extra Flax and Optax Tidbits</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a><ul><li><a href=#acknowledgements-and-extra-resources aria-label="Acknowledgements and Extra Resources">Acknowledgements and Extra Resources</a></li></ul></li></ul></div></details></div><div class=post-content><p>In my previous blog post, I discussed JAX - a framework for high performance
numerical computing and machine learning - in an atypical manner. <strong>I didn&rsquo;t
create a single training loop</strong>, and only showed a couple patterns that looked
vaguely machine learning-like. If you haven&rsquo;t read that blog post yet, you can
read it <a href=https://afmck.in/posts/2023-05-22-jax-post/>here</a>.</p><p>This approach was deliberate as I felt that JAX - although designed for machine
learning research - is more general-purpose than that. The steps to use it are
to define what you want to happen, wrap it in within <code>jax.jit</code>, let JAX trace
out your function into an intermediate graph representation, which is then
passed to XLA where it will be compiled and optimised. The result is a single,
heavily-optimised, binary blob, ready and waiting to receive your data. This
approach is a natural fit for many machine learning applications, as well as
other scientific computing tasks. Therefore, targeting machine learning only
didn&rsquo;t make sense. It is also ground that has already been extensively covered</p><ul><li>I wanted to do a different take on introductory JAX.</li></ul><p>In the previous post, I mentioned that it <em>is</em> possible to develop a full
machine learning training loop - models, optimisers and all - in pure JAX. This
is self-evident as JAX is general-purpose. It is a good exercise, but not a
strategy I like to employ. In this blog post I want to introduce two higher
level libraries built on top of JAX, that do a lot of the heavy lifting for us
when writing machine learning applications. These libraries are <strong>Flax</strong> and <strong>Optax</strong>.</p><p>To summarise the libraries:</p><ul><li><strong>JAX</strong> - provides a <strong>high-level neural network API</strong> that lets the developer
reason about the model in terms of components, like in PyTorch, rather than
with JAX functions that take parameters as inputs.</li><li><strong>Optax</strong> - a library containing a vast array of model training utilities, such
as <strong>optimisers, loss functions, learning rate schedulers</strong>, and more! Very
batteries-included.</li></ul><p>At the end of this post, we will have implemented and trained a very simple
<strong>class-conditioned image generation model</strong> called a <strong>variational
autoencoder</strong> (VAE) to generate MNIST digits.</p><h2 id=neural-network-api-with-flax>Neural Network API with Flax<a hidden class=anchor aria-hidden=true href=#neural-network-api-with-flax>#</a></h2><p>The high level structure of a training loop in pure JAX, looks something like
this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dataset <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>   <span style=color:#75715e># initialise training dataset that we can iterate over</span>
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>    <span style=color:#75715e># initialise trainable parameters of our model</span>
</span></span><span style=display:flex><span>epochs <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>model_forward</span>(params, batch):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>         <span style=color:#75715e># perform a forward pass of our model on `batch` using `params`</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> outputs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>loss_fn</span>(params, batch):
</span></span><span style=display:flex><span>    model_output <span style=color:#f92672>=</span> model_forward(params, batch)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>  <span style=color:#75715e># compute a loss based on `batch` and `model_output`</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@jax.jit</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(params, batch):
</span></span><span style=display:flex><span>    loss, grads <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>value_and_grad(loss_fn)(params, batch)
</span></span><span style=display:flex><span>    grads <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>  <span style=color:#75715e># transform `grads` (clipping, multiply by learning rate, etc.)</span>
</span></span><span style=display:flex><span>    params <span style=color:#f92672>=</span> <span style=color:#f92672>...</span> <span style=color:#75715e># update `params` using `grads` (such as via SGD)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> params, loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> dataset:
</span></span><span style=display:flex><span>        params, loss <span style=color:#f92672>=</span> train_step(params, batch)
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>         <span style=color:#75715e># report metrics like loss, accuracy, and the like.</span>
</span></span></code></pre></div><p>We define our model in a functional manner: a function that takes the model
parameters and a batch as input, and returns the output of the model.
Similarly, we define the loss function that also takes the parameters and a
batch as input, but returns the loss instead.</p><p>Our final function is the train step itself which we wrap in <code>jax.jit</code> – giving
XLA maximum context to compile and optimise the training step. This first
computes the gradient of the loss function using the function transform
<code>jax.value_and_grad</code>, manipulates the returned gradients (perhaps scaling by a
learning rate), and updates the parameters. We return the new parameters, and
use them on the next call to <code>train_step</code>. This is called in a loop, fetching
new batches from the dataset before each training step.</p><p>Most machine learning programs follow a pattern such as the one above. But in
frameworks like PyTorch, we package together the model forward pass and the
management of model parameters into a stateful object representing our model –
simplifying the training loop. It would be nice if we could imitate this
behaviour in stateless JAX to allow the developer to reason about models in a
class-based way. This is what Flax&rsquo;s neural network API – <code>flax.linen</code> – aims
to achieve.</p><blockquote><p>Whether or not writing models in a purely stateless, functional way is better
than a stateful, class-based way, is not the topic of this blog post. Both have
merits. <strong>Regardless, during execution the final result is the same whether we
use Flax or not. We get a stateless, heavily-optimised, binary blob that we
throw data at.</strong> It&rsquo;s all JAX after all.</p></blockquote><p>There are two main ways to define a module in Flax: one is PyTorch-like and
the other is a compact representation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> flax.linen <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Callable
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Model</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    dim: int
</span></span><span style=display:flex><span>    activation_fn: Callable <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>relu
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>setup</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(self<span style=color:#f92672>.</span>dim)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __call__(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>activation_fn(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ModelCompact</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    dim: int
</span></span><span style=display:flex><span>    activation_fn: Callable <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>relu
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@nn.compact</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __call__(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(self<span style=color:#f92672>.</span>dim)(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>activation_fn(x)     
</span></span></code></pre></div><p>If we have complex initialisation logic, the former may be more appropriate. If
the module is relatively simple, we can make use of the <code>nn.compact</code>
representation to automatically define the module by the forward pass alone.</p><p>Like other frameworks, we can nest modules within each other to implement
complex model behaviour. Like we&rsquo;ve already seen, <code>flax.linen</code> provides some
pre-baked modules like <code>nn.Dense</code> (same as PyTorch&rsquo;s <code>nn.Linear</code>). I won&rsquo;t
enumerate them all, but the usual candidates are all there like convolutions,
embeddings, and more.</p><blockquote><p>Something to bear in mind if you are porting models from PyTorch to Flax is
that the default weight initialisation may be different. For example, in
PyTorch the default bias initialisation is the LeCun normal, but in Flax it is
initialised to zero.</p></blockquote><p>However, currently we cannot call this model, even if we were to initialise the
class itself. There simply aren&rsquo;t any parameters to use. Furthermore, the
module is never a container for parameters. <strong>An instance of a Flax module is
simply a hollow shell, that loosely associates operations with parameters and
inputs</strong> that are passed as input later.</p><p>To see what I mean, let&rsquo;s initialise some parameters for our model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0xffff</span>)
</span></span><span style=display:flex><span>key, model_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Model(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>init(model_key, jnp<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>8</span>)))
</span></span><span style=display:flex><span>params
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: 
</span></span><span style=display:flex><span>FrozenDict({
</span></span><span style=display:flex><span>    params: {
</span></span><span style=display:flex><span>        layer: {
</span></span><span style=display:flex><span>            kernel: Array([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.05412389</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.28172645</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.07438638</span>,  <span style=color:#ae81ff>0.5238516</span> ],
</span></span><span style=display:flex><span>                   [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.13562573</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.17592733</span>,  <span style=color:#ae81ff>0.45305118</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0650041</span> ],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.25177842</span>,  <span style=color:#ae81ff>0.13981569</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.41496065</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.15681015</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.13783392</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.6254694</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>0.09966562</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.04283331</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.48194656</span>,  <span style=color:#ae81ff>0.07596914</span>,  <span style=color:#ae81ff>0.0429794</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2127948</span> ],
</span></span><span style=display:flex><span>                   [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.6694777</span> ,  <span style=color:#ae81ff>0.15849823</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4057232</span> ,  <span style=color:#ae81ff>0.26767966</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.22948688</span>,  <span style=color:#ae81ff>0.00706845</span>,  <span style=color:#ae81ff>0.0145666</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1280596</span> ],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.62309605</span>,  <span style=color:#ae81ff>0.12575962</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.05112049</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.316764</span>  ]],      dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>            bias: Array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>})
</span></span></code></pre></div><p>In the above cell, we first initialised our model class, which returns an
instance of <code>Model</code> which we assign to the variable <code>model</code>. Like I said, it
does not contain any parameters, it is just a hollow shell that we pass
parameters and inputs to. We can see this by printing the <code>model</code> variable
itself:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: Model(
</span></span><span style=display:flex><span>    <span style=color:#75715e># attributes</span>
</span></span><span style=display:flex><span>    dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>    activation_fn <span style=color:#f92672>=</span> relu
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>We can also call the module itself, which will fail even though we have defined
the <code>__call__</code> method:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model(jnp<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>8</span>)))
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out:
</span></span><span style=display:flex><span><span style=color:#f92672>/</span>usr<span style=color:#f92672>/</span>local<span style=color:#f92672>/</span>lib<span style=color:#f92672>/</span>python3<span style=color:#ae81ff>.10</span><span style=color:#f92672>/</span>dist<span style=color:#f92672>-</span>packages<span style=color:#f92672>/</span>flax<span style=color:#f92672>/</span>linen<span style=color:#f92672>/</span>module<span style=color:#f92672>.</span>py <span style=color:#f92672>in</span> __getattr__(self, name)
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>935</span>         msg <span style=color:#f92672>+=</span> (<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39; If &#34;</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34; is defined in </span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74>.setup()</span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74>, remember these fields &#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>936</span>           <span style=color:#e6db74>&#39;are only accessible from inside </span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74>init</span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74> or </span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74>apply</span><span style=color:#ae81ff>\&#39;</span><span style=color:#e6db74>.&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>--&gt;</span> <span style=color:#ae81ff>937</span>       <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>AttributeError</span>(msg)
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>938</span> 
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>939</span>   <span style=color:#66d9ef>def</span> __dir__(self) <span style=color:#f92672>-&gt;</span> List[str]:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>AttributeError</span>: <span style=color:#e6db74>&#34;Model&#34;</span> object has no attribute <span style=color:#e6db74>&#34;layer&#34;</span><span style=color:#f92672>.</span> If <span style=color:#e6db74>&#34;layer&#34;</span> <span style=color:#f92672>is</span> defined <span style=color:#f92672>in</span> <span style=color:#e6db74>&#39;.setup()&#39;</span>, remember these fields are only accessible <span style=color:#f92672>from</span> inside <span style=color:#e6db74>&#39;init&#39;</span> <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;apply&#39;</span><span style=color:#f92672>.</span>
</span></span></code></pre></div><p>To initialise the parameters, we passed a PRNG key and some dummy inputs to the
model&rsquo;s <code>init</code> function of the same shape and dtype as the inputs we will use
later. In this simple case, we just pass <code>x</code> as in the original module&rsquo;s
<code>__call__</code> definition, but could be multiple arrays, PyTrees, or PRNG keys. We
need the input shapes and dtypes in order to determine the shape and dtype of
the model parameters.</p><p>From the <code>model.init</code> call, we get a nested <code>FrozenDict</code> holding our model&rsquo;s
parameters. If you have seen PyTorch state dictionaries, the format of the
parameters is similar: nested dictionaries with meaningful named keys, with
parameter arrays as values. If you&rsquo;ve read my previous blog post or read about
JAX before, you will know that this structure is a PyTree. Not only does Flax
help developers loosely associate parameters and operations, <strong>it also helps
initialise model parameters based on the model definition</strong>.</p><p>With the parameters, we can call the model using <code>model.apply</code> – providing the
parameters and inputs:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>key, x_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(x_key, (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>8</span>))
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>apply(params, x)
</span></span><span style=display:flex><span>y
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: Array([[<span style=color:#ae81ff>0.9296505</span> , <span style=color:#ae81ff>0.25998798</span>, <span style=color:#ae81ff>0.01101626</span>, <span style=color:#ae81ff>0.</span>        ]], dtype<span style=color:#f92672>=</span>float32)
</span></span></code></pre></div><p>There is nothing special about the PyTree returned by <code>model.init</code> – it is just
a regular PyTree storing the model&rsquo;s parameters. <code>params</code> can be swapped with
any other PyTree that contains the parameters <code>model</code> expects:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>zero_params <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>tree_map(jnp<span style=color:#f92672>.</span>zeros_like, params) <span style=color:#75715e># generates a PyTree with same structure as `params` will all values set to 0.</span>
</span></span><span style=display:flex><span>print(zero_params)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>apply(zero_params, x)
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out:
</span></span><span style=display:flex><span>FrozenDict({
</span></span><span style=display:flex><span>    params: {
</span></span><span style=display:flex><span>        layer: {
</span></span><span style=display:flex><span>            bias: Array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>            kernel: Array([[<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>]], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Array([[<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>]], dtype<span style=color:#f92672>=</span>float32)
</span></span></code></pre></div><p>Forcing model calls to require explicitly passing parameters keeps it
stateless and returning parameters like any other PyTree, makes Flax
interoperable with JAX functions – as well as other libraries built on JAX.
<strong>Essentially, by using Flax we aren&rsquo;t forced to use any other specific
frameworks and have access to all regular JAX features.</strong></p><p>If you are used to frameworks like PyTorch, calling models like this feels
unnatural at first. However, I personally quite like it this way – it feels
rather elegant to pass different parameters to the model to get different
behaviour rather than &ldquo;load&rdquo; the weights. A bit subjective and fuzzy, I know,
but I like it.</p><blockquote><p>To summarise the difference, if we aim to implement $f_\theta(x)$, a PyTorch
module is basically $f_\theta$ (which we can call on $x$). A Flax module is
simply $f$, which needs to be provided parameters $\theta$ before it can be
called on $x$ – or alternatively, we call $f$ on $(\theta, x)$.</p></blockquote><p>All in all, the point of Flax is to <strong>provide a familiar stateful API for
development</strong> whilst <strong>preserving JAX statelessness during runtime</strong>. We can
build our neural network modules in terms of classes and objects, but <strong>the
final result is a stateless function <code>model.apply</code> that takes in our inputs and
a PyTree of parameters.</strong></p><p>This is identical behaviour to what we began with (recall our <code>model_forward</code>
function at the start of this section), just now tied up nicely together.
Therefore, our function containing <code>model.apply</code> that takes as input our
PyTree, can be safely jit-compiled. The result is the same, a heavily-optimised
binary blob we bombard with data. Nothing changes during runtime, it just makes
development easier for those who prefer reasoning about neural networks in a
class-based way whilst remaining interoperable with, and keeping the
performance of JAX.</p><p>There&rsquo;s a lot more to Flax than this, especially outside the <code>flax.linen</code>
neural network API. For now though, we will move on to developing a full
training loop using Flax and <strong>Optax</strong>. We will swing back around to some extra
Flax points later, but I feel some concepts are hard to explain without first
showing a training loop.</p><h2 id=a-full-training-loop-with-optax-and-flax>A full training loop with Optax and Flax<a hidden class=anchor aria-hidden=true href=#a-full-training-loop-with-optax-and-flax>#</a></h2><p>We&rsquo;ve shown how to reduce the complexity of writing model code and parameter
initialisation. We can push this further by relying on Optax to handle the
gradient manipulation and parameter updates in <code>train_step</code>. For simple
optimisers, these steps can be quite simple. However, for more complex
optimisers or gradient transformation behaviour, it can get quite complex to
implement in JAX alone. Optax packages this complex behaviour into a simple
API.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> optax
</span></span><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>sgd(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>)
</span></span><span style=display:flex><span>optimiser
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: GradientTransformationExtraArgs(init<span style=color:#f92672>=&lt;</span>function chain<span style=color:#f92672>.&lt;</span>locals<span style=color:#f92672>&gt;.</span>init_fn at <span style=color:#ae81ff>0x7fa7185503a0</span><span style=color:#f92672>&gt;</span>, update<span style=color:#f92672>=&lt;</span>function chain<span style=color:#f92672>.&lt;</span>locals<span style=color:#f92672>&gt;.</span>update_fn at <span style=color:#ae81ff>0x7fa718550550</span><span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><p>Not pretty, but we can see that the optimiser is just a <strong>gradient
transformation</strong> – in fact all optimisers in Optax are implemented as gradient
transformations. A gradient transformation is defined to be a pair of functions
<code>init</code> and <code>update</code>, which are both pure functions. Like a Flax model, Optax
optimisers have no state kept internally, and must be initialised before it can
be used, and any state must be passed by the developer to <code>update</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimiser_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>init(params)
</span></span><span style=display:flex><span>optimiser_state
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: (EmptyState(), EmptyState())
</span></span></code></pre></div><p>Of course, as SGD is a stateless optimiser, the initialisation call simply
returns an empty state. It must return this to maintain the API of a gradient
transformation. Let&rsquo;s try with a more complex optimiser like Adam:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>adam(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>)
</span></span><span style=display:flex><span>optimiser_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>init(params)
</span></span><span style=display:flex><span>optimiser_state
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: (ScaleByAdamState(count<span style=color:#f92672>=</span>Array(<span style=color:#ae81ff>0</span>, dtype<span style=color:#f92672>=</span>int32), mu<span style=color:#f92672>=</span>FrozenDict({
</span></span><span style=display:flex><span>     params: {
</span></span><span style=display:flex><span>         layer: {
</span></span><span style=display:flex><span>             bias: Array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>             kernel: Array([[<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>]], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>         },
</span></span><span style=display:flex><span>     },
</span></span><span style=display:flex><span> }), nu<span style=color:#f92672>=</span>FrozenDict({
</span></span><span style=display:flex><span>     params: {
</span></span><span style=display:flex><span>         layer: {
</span></span><span style=display:flex><span>             bias: Array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>             kernel: Array([[<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>                    [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>]], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>         },
</span></span><span style=display:flex><span>     },
</span></span><span style=display:flex><span> })),
</span></span><span style=display:flex><span> EmptyState())
</span></span></code></pre></div><p>Here, we can see the first and second order statistics of the Adam optimiser,
as well as a count storing number of optimiser updates. Like with SGD, this
state needs to be passed to <code>update</code> when called.</p><blockquote><p>Like Flax parameters, the optimiser state is just a PyTree. Any
PyTree with a compatible structure could also be used. Again, this also allows
interoperability with JAX and <code>jax.jit</code>, as well as other libraries built on
top of JAX.</p></blockquote><p>Concretely, <strong>Optax gradient transformations are simply a named tuple
containing pure functions <code>init</code> and <code>update</code></strong>. <code>init</code> is a pure function which
takes in an example instance of gradients to be transformed and returns the
optimiser initial state. In the case of <code>optax.sgd</code> this returns an empty state
regardless of the example provided. For <code>optax.adam</code>, we get a more complex
state containing the first and second order statistics of the same PyTree
structure as the provided example.</p><p><code>update</code> takes in a PyTree of updates with the same structure as the example
instance provided to <code>init</code>. In addition, it takes in the optimiser state
returned by <code>init</code> and optionally the parameters of the model itself, which may
be needed for some optimisers. This function will return the transformed
gradients (<strong>which could be another set of gradients, or the actual parameter
updates</strong>) and the new optimiser state.</p><blockquote><p>This is explained quite nicely in the documentation
<a href="https://optax.readthedocs.io/en/latest/api.html?highlight=gradienttransform#optax-types">here</a></p></blockquote><p>In action on some dummy data, we get the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> optax
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2.0</span>]) <span style=color:#75715e># some dummy parameters</span>
</span></span><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>adam(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>opt_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>init(params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>grads <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>4.0</span>, <span style=color:#ae81ff>0.6</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>])<span style=color:#75715e># some dummy gradients</span>
</span></span><span style=display:flex><span>updates, opt_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>update(grads, opt_state, params)
</span></span><span style=display:flex><span>updates
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: Array([<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00999993</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00999993</span>,  <span style=color:#ae81ff>0.00999993</span>], dtype<span style=color:#f92672>=</span>float32)
</span></span></code></pre></div><p>Optax provides a helper function to actually apply the updates to our
parameters:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>new_params <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>apply_updates(params, updates)
</span></span><span style=display:flex><span>new_params
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: Array([<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00999993</span>,  <span style=color:#ae81ff>0.99000007</span>,  <span style=color:#ae81ff>2.01</span>      ], dtype<span style=color:#f92672>=</span>float32)
</span></span></code></pre></div><p>It is important to emphasise that Optax optimisers are gradient transformations,
<strong>but gradient transformations are not just optimisers.</strong> We&rsquo;ll see more of that
later after we finish the training loop.</p><p>On that note, let&rsquo;s begin with said training loop. Recall that our goal is to
train a class-conditioned, variational autoencoder (VAE) on the MNIST dataset.</p><blockquote><p>I chose this example as it is slightly more interesting than the typical
classification example found in most tutorials.</p></blockquote><p>Not strictly related to JAX, Flax, or Optax, but it is worth describing what a
VAE is. First, an autoencoder model is one that maps some input $x$ in our data
space to a <strong>latent vector</strong> $z$ in the <strong>latent space</strong> (a space with smaller
dimensionality than the data space) and back to the data space. It is trained to
minimise the reconstruction loss between the input and the output, essentially
learning the identity function through an <strong>information bottleneck</strong>.</p><p>The portion of the network that maps from the data space to the latent space is
called the <strong>encoder</strong> and the portion that maps from the latent space to the
data space is called the <strong>decoder</strong>. Applying the encoder is somewhat
analogous to lossy compression. Likewise, *applying the decoder is akin to
lossy decompression.</p><p>What makes a VAE different to an autoencoder is that the encoder does not
output the latent vector directly. Instead, <strong>it outputs the mean and
log-variance of a Gaussian distribution, which we then sample from in order
to obtain our latent vector</strong>. We apply an extra loss term to make these mean and
log-variance outputs roughly follow the standard normal distribution.</p><blockquote><p>Interestingly, defining the encoder this way means for every given input $x$
we have many possible latent vectors which are sampled stochastically. Our
encoder is almost mapping to a sphere of possible latents centred at the mean
vector with radius scaling with log-variance.</p></blockquote><p>The decoder is the same as before. However, now we can sample <strong>a latent from
the normal distribution and pass it to the decoder in order to generate samples
like those in the dataset</strong>! Adding the variational component turns our
autoencoder compression model into a VAE generative model.</p><p>Our goal is to implement the model code for the VAE as well as the training loop
with both the reconstruction and variational loss terms. Then, we can sample new
digits that look like those in the MNIST dataset! Additionally, we will provide
an extra input to the model – the class index – so we can control which number
we want to generate.</p><p>Let&rsquo;s begin by defining our configuration. For this educational example, we will
just define some constants in a cell:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>latent_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>kl_weight <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>num_classes <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>seed <span style=color:#f92672>=</span> <span style=color:#ae81ff>0xffff</span>
</span></span></code></pre></div><p>Along with some imports and PRNG initialisation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax <span style=color:#75715e># install correct wheel for accelerator you want to use</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> flax
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> optax
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> orbax
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> flax.linen <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> jax.numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> jax.typing <span style=color:#f92672>import</span> ArrayLike
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Tuple, Callable
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> math <span style=color:#f92672>import</span> sqrt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchvision.transforms <span style=color:#66d9ef>as</span> T
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision.datasets <span style=color:#f92672>import</span> MNIST
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(seed)
</span></span></code></pre></div><p>Let&rsquo;s grab our MNIST dataset while we are here:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> MNIST(<span style=color:#e6db74>&#39;data&#39;</span>, train <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>, transform<span style=color:#f92672>=</span>T<span style=color:#f92672>.</span>ToTensor(), download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>train_loader <span style=color:#f92672>=</span> DataLoader(train_dataset, batch_size<span style=color:#f92672>=</span>batch_size, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, drop_last<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><blockquote><p>JAX, Flax, and Optax do not have data loading utilities, so I just use
the perfectly serviceable PyTorch implementation of the MNIST dataset here.</p></blockquote><p>Now to our first real Flax model. We begin by defining a submodule <code>FeedForward</code>
that implements a stack of linear layers with intermediate non-linearities:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FeedForward</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>  dimensions: Tuple[int] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>64</span>)
</span></span><span style=display:flex><span>  activation_fn: Callable <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>relu
</span></span><span style=display:flex><span>  drop_last_activation: bool <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@nn.compact</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> __call__(self, x: ArrayLike) <span style=color:#f92672>-&gt;</span> ArrayLike:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, d <span style=color:#f92672>in</span> enumerate(self<span style=color:#f92672>.</span>dimensions):
</span></span><span style=display:flex><span>      x <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(d)(x)  
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> i <span style=color:#f92672>!=</span> len(self<span style=color:#f92672>.</span>dimensions) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> <span style=color:#f92672>not</span> self<span style=color:#f92672>.</span>drop_last_activation:
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation_fn(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key, model_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> FeedForward(dimensions <span style=color:#f92672>=</span> (<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>), drop_last_activation <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>init(model_key, jnp<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>8</span>)))
</span></span><span style=display:flex><span>print(params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key, x_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(x_key, (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>8</span>))
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>apply(params, x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FeedForward(
</span></span><span style=display:flex><span>    <span style=color:#75715e># attributes</span>
</span></span><span style=display:flex><span>    dimensions <span style=color:#f92672>=</span> (<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    activation_fn <span style=color:#f92672>=</span> relu
</span></span><span style=display:flex><span>    drop_last_activation <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>FrozenDict({
</span></span><span style=display:flex><span>    params: {
</span></span><span style=display:flex><span>        Dense_0: {
</span></span><span style=display:flex><span>            kernel: Array([[ <span style=color:#ae81ff>0.0840368</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>0.18825287</span>,  <span style=color:#ae81ff>0.49946404</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4610112</span> ],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.4370267</span> ,  <span style=color:#ae81ff>0.21035315</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.19604324</span>,  <span style=color:#ae81ff>0.39427406</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.00632685</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.02732705</span>,  <span style=color:#ae81ff>0.16799504</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.44181877</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.26044282</span>,  <span style=color:#ae81ff>0.42476758</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.14758752</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.29886967</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.57811564</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.18126923</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.19411889</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.10860331</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.20605426</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.16065307</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3016759</span> ,  <span style=color:#ae81ff>0.44704655</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.35531637</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.14256613</span>,  <span style=color:#ae81ff>0.13841921</span>,  <span style=color:#ae81ff>0.11269159</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.430825</span>  , <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0171169</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>0.52949774</span>,  <span style=color:#ae81ff>0.4862139</span> ]],      dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>            bias: Array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        Dense_1: {
</span></span><span style=display:flex><span>            kernel: Array([[ <span style=color:#ae81ff>0.03389561</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00805947</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.47362345</span>,  <span style=color:#ae81ff>0.37944487</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.41766328</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.15580587</span>],
</span></span><span style=display:flex><span>                   [ <span style=color:#ae81ff>0.5538078</span> ,  <span style=color:#ae81ff>0.18003668</span>]], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>            bias: Array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        Dense_2: {
</span></span><span style=display:flex><span>            kernel: Array([[ <span style=color:#ae81ff>1.175035</span> ],
</span></span><span style=display:flex><span>                   [<span style=color:#f92672>-</span><span style=color:#ae81ff>1.1607001</span>]], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>            bias: Array([<span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Array([[<span style=color:#ae81ff>0.5336972</span>]], dtype<span style=color:#f92672>=</span>float32)
</span></span></code></pre></div><p>We use the <code>nn.compact</code> decorator here as the logic is relatively simple. We
iterate over the tuple <code>self.dimensions</code> and pass our current activations
through a <code>nn.Dense</code> module, followed by applying <code>self.activation_fn</code>. This
activation can optionally be dropped for the final linear layer in
<code>FeedForward</code>. This is needed as <code>nn.relu</code> only outputs non-negative values,
whereas sometimes we need non-negative outputs!</p><p>Using <code>FeedForward</code>, we can define our full VAE model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>VAE</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>  encoder_dimensions: Tuple[int] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>64</span>)
</span></span><span style=display:flex><span>  decoder_dimensions: Tuple[int] <span style=color:#f92672>=</span> (<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>784</span>)
</span></span><span style=display:flex><span>  latent_dim: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>  activation_fn: Callable <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>relu
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>setup</span>(self):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> FeedForward(self<span style=color:#f92672>.</span>encoder_dimensions, self<span style=color:#f92672>.</span>activation_fn)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>pre_latent_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(self<span style=color:#f92672>.</span>latent_dim <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>post_latent_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(self<span style=color:#f92672>.</span>encoder_dimensions[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>class_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(self<span style=color:#f92672>.</span>encoder_dimensions[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>decoder <span style=color:#f92672>=</span> FeedForward(self<span style=color:#f92672>.</span>decoder_dimensions, self<span style=color:#f92672>.</span>activation_fn, drop_last_activation<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>reparam</span>(self, mean: ArrayLike, logvar: ArrayLike, key: jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey) <span style=color:#f92672>-&gt;</span> ArrayLike:
</span></span><span style=display:flex><span>    std <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>exp(logvar <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>    eps <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(key, mean<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> eps <span style=color:#f92672>*</span> std <span style=color:#f92672>+</span> mean
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>encode</span>(self, x: ArrayLike):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(x)
</span></span><span style=display:flex><span>    mean, logvar <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>split(self<span style=color:#f92672>.</span>pre_latent_proj(x), <span style=color:#ae81ff>2</span>, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mean, logvar
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>decode</span>(self, x: ArrayLike, c: ArrayLike):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>post_latent_proj(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>class_proj(c)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decoder(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> __call__(
</span></span><span style=display:flex><span>      self, x: ArrayLike, c: ArrayLike, key: jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey) <span style=color:#f92672>-&gt;</span> Tuple[ArrayLike, ArrayLike, ArrayLike]:
</span></span><span style=display:flex><span>    mean, logvar <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encode(x)
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>reparam(mean, logvar, key)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decode(z, c)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> y, mean, logvar
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0x1234</span>)
</span></span><span style=display:flex><span>key, model_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> VAE(latent_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>print(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key, call_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>init(model_key, jnp<span style=color:#f92672>.</span>zeros((batch_size, <span style=color:#ae81ff>784</span>)), jnp<span style=color:#f92672>.</span>zeros((batch_size, num_classes)), call_key)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>recon, mean, logvar <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>apply(params, jnp<span style=color:#f92672>.</span>zeros((batch_size, <span style=color:#ae81ff>784</span>)), jnp<span style=color:#f92672>.</span>zeros((batch_size, num_classes)), call_key)
</span></span><span style=display:flex><span>recon<span style=color:#f92672>.</span>shape, mean<span style=color:#f92672>.</span>shape, logvar<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out:
</span></span><span style=display:flex><span>ClassVAE(
</span></span><span style=display:flex><span>    <span style=color:#75715e># attributes</span>
</span></span><span style=display:flex><span>    encoder_dimensions <span style=color:#f92672>=</span> (<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>64</span>)
</span></span><span style=display:flex><span>    decoder_dimensions <span style=color:#f92672>=</span> (<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>784</span>)
</span></span><span style=display:flex><span>    latent_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>    activation_fn <span style=color:#f92672>=</span> relu
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>((<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>784</span>), (<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>4</span>), (<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>4</span>))
</span></span></code></pre></div><p>There is a lot to the above cell. Knowing the specifics of how this model works
isn&rsquo;t too important to understanding the training loop later, as we can treat
the model as a bit of a black box. Simply substitute your own model of choice.
Saying that, I&rsquo;ll unpack each function briefly:</p><ul><li><code>setup</code>: Creates the submodules of the network, namely two <code>FeedForward</code>
stacks and two <code>nn.Linear</code> layers that project to and from the latent space.
Additionally, it initialises a third <code>nn.Linear</code> layer that projects our
class conditioning vector to the same dimensionality as the last encoder
layer.</li><li><code>reparam</code>: Sampling a latent directly from a random Gaussian is not
differentiable, hence we employ the <strong>reparameterisation trick</strong>. This
involves sampling a random vector, scaling by the standard deviation, then
adding to the mean. As it involves random array generation, we take as input
a key in addition to the mean and log-variance.</li><li><code>encode</code>: Applies the encoder and projection to the latent space to the
input. Note, the output of the projection is actually double the size of the
latent space, as we split it in twine to obtain our mean and log-variance.</li><li><code>decode</code>: Applies a projection from the latent space to <code>x</code>, followed by
adding the output of <code>class_proj</code> on the conditioning vector. Finally, it
passes the result through the decoder stack.</li><li><code>__call__</code>: This is simply the full model forward pass: <code>encode</code> then
<code>reparam</code> then <code>decode</code>. This is used during training.</li></ul><p>The above example also demonstrates that we can add other functions to our Flax
modules aside from <code>setup</code> and <code>__call__</code>. This is useful for more complex
behaviour, or if we want to only execute parts of the model (more on this
later).</p><p>We now have our model, optimiser, and dataset. The next step is to write the
function that implements our training step and then jit-compile it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_train_step</span>(key, model, optimiser):
</span></span><span style=display:flex><span>  params <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>init(key, jnp<span style=color:#f92672>.</span>zeros((batch_size, <span style=color:#ae81ff>784</span>)), jnp<span style=color:#f92672>.</span>zeros((batch_size, num_classes)), jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0</span>)) <span style=color:#75715e># dummy key just as example input</span>
</span></span><span style=display:flex><span>  opt_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>init(params)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>loss_fn</span>(params, x, c, key):
</span></span><span style=display:flex><span>    reduce_dims <span style=color:#f92672>=</span> list(range(<span style=color:#ae81ff>1</span>, len(x<span style=color:#f92672>.</span>shape)))
</span></span><span style=display:flex><span>    c <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>one_hot(c, num_classes) <span style=color:#75715e># one hot encode the class index</span>
</span></span><span style=display:flex><span>    recon, mean, logvar <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>apply(params, x, c, key)
</span></span><span style=display:flex><span>    mse_loss <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>l2_loss(recon, x)<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span>reduce_dims)<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>    kl_loss <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>mean(<span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> jnp<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> logvar <span style=color:#f92672>-</span> mean <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> jnp<span style=color:#f92672>.</span>exp(logvar), axis<span style=color:#f92672>=</span>reduce_dims)) <span style=color:#75715e># KL loss term to keep encoder output close to standard normal distribution.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> mse_loss <span style=color:#f92672>+</span> kl_weight <span style=color:#f92672>*</span> kl_loss
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> loss, (mse_loss, kl_loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@jax.jit</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(params, opt_state, x, c, key):
</span></span><span style=display:flex><span>    losses, grads <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>value_and_grad(loss_fn, has_aux<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)(params, x, c, key)
</span></span><span style=display:flex><span>    loss, (mse_loss, kl_loss) <span style=color:#f92672>=</span> losses
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    updates, opt_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>update(grads, opt_state, params)
</span></span><span style=display:flex><span>    params <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>apply_updates(params, updates)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> params, opt_state, loss, mse_loss, kl_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> train_step, params, opt_state
</span></span></code></pre></div><p>Here, I don&rsquo;t define the training step directly, but rather first define a
function that returns the training step function given a target model and
optimiser, along with returning the freshly initialised parameters and
optimiser state.</p><p>Let us unpack it all:</p><ol><li>First, it initialises our model using an example input. In this case, this
is a 784-dim array which contains the (flattened) MNIST digit and a random,
random key.</li><li>Also initialises the optimiser state using the parameters we just
initialised.</li><li>Now, it defines the loss function. This is simply a <code>model.apply</code> call which
returns the model&rsquo;s reconstruction of the input, along with the predicted
mean and log-variance. We then compute the mean-squared error loss and the
KL-divergence, before finally computing a weighted sum to get our final
loss. The KL loss term is what keeps the encoder outputs close to a standard
normal distribution.</li><li>Next, the actual train step definition. This begins by transforming
<code>loss_fn</code> using our old friend <code>jax.value_and_grad</code> which will return the
loss and also the gradients. We must set <code>has_aux=True</code> as we return all
individual loss terms for logging purposes. We provide the gradients,
optimiser state, and parameters to <code>optimiser.update</code> which returns the
transformed gradients and the new optimiser state. The transformed gradients
are then applied to the parameters. Finally, we return the new parameters,
optimiser state, and loss terms – followed by wrapping the whole thing in
<code>jax.jit</code>. Phew..</li></ol><blockquote><p>A function that generates the training step is just a pattern I quite like,
and there is nothing stopping you from just writing the training step directly.</p></blockquote><p>Let&rsquo;s call <code>create_train_step</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>key, model_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> VAE(latent_dim<span style=color:#f92672>=</span>latent_dim)
</span></span><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>adamw(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-4</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_step, params, opt_state <span style=color:#f92672>=</span> create_train_step(model_key, model, optimiser)
</span></span></code></pre></div><p>When we call the above, we get a <code>train_step</code> ready to be compiled and accept
our parameters, optimiser state, and data at blistering fast speeds. As always
with jit-compiled functions, the first call with a given set of input shapes
will be slow, but fast on subsequent calls as we skip the compiling and
optimisation process.</p><p>We are now in a position to write our training loop and train the model!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>freq <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>  total_loss, total_mse, total_kl <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> i, (batch, c) <span style=color:#f92672>in</span> enumerate(train_loader):
</span></span><span style=display:flex><span>    key, subkey <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    batch <span style=color:#f92672>=</span> batch<span style=color:#f92672>.</span>numpy()<span style=color:#f92672>.</span>reshape(batch_size, <span style=color:#ae81ff>784</span>)
</span></span><span style=display:flex><span>    c <span style=color:#f92672>=</span> c<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>    params, opt_state, loss, mse_loss, kl_loss <span style=color:#f92672>=</span> train_step(params, opt_state, batch, c, subkey)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    total_loss <span style=color:#f92672>+=</span> loss
</span></span><span style=display:flex><span>    total_mse <span style=color:#f92672>+=</span> mse_loss
</span></span><span style=display:flex><span>    total_kl <span style=color:#f92672>+=</span> kl_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> i <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> i <span style=color:#f92672>%</span> freq:
</span></span><span style=display:flex><span>      print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74> | step </span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74> | loss: </span><span style=color:#e6db74>{</span>total_loss <span style=color:#f92672>/</span> freq<span style=color:#e6db74>}</span><span style=color:#e6db74> ~ mse: </span><span style=color:#e6db74>{</span>total_mse <span style=color:#f92672>/</span> freq<span style=color:#e6db74>}</span><span style=color:#e6db74>. kl: </span><span style=color:#e6db74>{</span>total_kl <span style=color:#f92672>/</span> freq<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>      total_loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>      total_mse, total_kl <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out:
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>0</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>100</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>49.439998626708984</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>49.060447692871094</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>0.7591156363487244</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>0</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>200</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>37.1823616027832</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>36.82903289794922</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>0.7066375613212585</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>0</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>300</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>33.82365036010742</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>33.49456024169922</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>0.6581906080245972</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>0</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>400</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>31.904821395874023</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>31.570871353149414</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>0.6679074764251709</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>0</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>500</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>31.095705032348633</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>30.763246536254883</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>0.6649144887924194</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>0</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>600</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>29.771989822387695</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>29.42426872253418</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>0.6954278349876404</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>3100</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>14.035745620727539</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>10.833460807800293</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>6.404574871063232</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>3200</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>14.31241226196289</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>11.043667793273926</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>6.53748893737793</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>3300</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>14.26440143585205</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>11.01070785522461</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>6.5073771476745605</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>3400</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>13.96005630493164</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>10.816412925720215</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>6.28728723526001</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>3500</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>14.166285514831543</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>10.919700622558594</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>6.493169784545898</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>3600</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>13.819541931152344</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>10.632755279541016</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>6.373570919036865</span>
</span></span><span style=display:flex><span>epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>|</span> step <span style=color:#ae81ff>3700</span> <span style=color:#f92672>|</span> loss: <span style=color:#ae81ff>14.452215194702148</span> <span style=color:#f92672>~</span> mse: <span style=color:#ae81ff>11.186063766479492</span><span style=color:#f92672>.</span> kl: <span style=color:#ae81ff>6.532294750213623</span>
</span></span></code></pre></div><p>Now that we have our <code>train_step</code> function, the training loop itself is just
repeatedly fetching data, calling our uber-fast <code>train_step</code> function, and
logging results so we can track training. We can see that the loss is
decreasing, which means our model is training!</p><blockquote><p>Note that the KL-loss term <em>increases</em> during training. This is okay
so long as it doesn&rsquo;t get too high, in which case sampling from the model
becomes impossible. Tuning the hyperparameter <code>kl_weight</code> is quite important.
Too low and we get perfect reconstructions but no sampling capabilities – too
high and the outputs will become blurry.</p></blockquote><p>Let&rsquo;s sample from the model so we can see that it does indeed produce some
reasonable samples:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_sample_fn</span>(model, params):
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@jax.jit</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sample_fn</span>(z: jnp<span style=color:#f92672>.</span>array, c: jnp<span style=color:#f92672>.</span>array) <span style=color:#f92672>-&gt;</span> jnp<span style=color:#f92672>.</span>array:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model<span style=color:#f92672>.</span>apply(params, z, c, method<span style=color:#f92672>=</span>model<span style=color:#f92672>.</span>decode)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> sample_fn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample_fn <span style=color:#f92672>=</span> build_sample_fn(model, params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_samples <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>h, w <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>key, z_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(z_key, (num_samples, latent_dim))
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>repeat(np<span style=color:#f92672>.</span>arange(h)[:, np<span style=color:#f92672>.</span>newaxis], w, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>flatten()
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>one_hot(c, num_classes)
</span></span><span style=display:flex><span>sample <span style=color:#f92672>=</span> sample_fn(z, c)
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>shape, c<span style=color:#f92672>.</span>shape, sample<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: ((<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>32</span>), (<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>784</span>))
</span></span></code></pre></div><p>The above cell generates 100 samples – 10 examples from each of the 10 classes.
We jit-compile our sample function in case we want to sample again later. We
only call the <code>model.decode</code> method, rather than the full model, as we only
need to decode our randomly sampled latents. This is achieved by specifying
<code>method=model.decode</code> in the <code>model.apply</code> call.</p><p>Let&rsquo;s visualise the results using matplotlib:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> numpy <span style=color:#f92672>import</span> einsum
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample <span style=color:#f92672>=</span> einsum(<span style=color:#e6db74>&#39;ikjl&#39;</span>, np<span style=color:#f92672>.</span>asarray(sample)<span style=color:#f92672>.</span>reshape(h, w, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>))<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>28</span><span style=color:#f92672>*</span>h, <span style=color:#ae81ff>28</span><span style=color:#f92672>*</span>w)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(sample, cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gray&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img loading=lazy src=img/sample.png alt="Sample MNIST digits from our trained model"></p><p>It seems our model did indeed train and can be sampled from! Additionally, the
model is capable of using the class conditioning signal so that we can control
which digits are generated. Therefore, we have succeeded in building a full
training loop using Flax and Optax!</p><h2 id=extra-flax-and-optax-tidbits>Extra Flax and Optax Tidbits<a hidden class=anchor aria-hidden=true href=#extra-flax-and-optax-tidbits>#</a></h2><p>I&rsquo;d like to finish this blog post by highlighting some interesting and useful
features that may prove useful in your own applications. I won&rsquo;t delve into
great detail but simply summarise and point you in the right direction.</p><p>You may have noticed already that when we add parameters, optimiser states, and
a bunch of other metrics to the return call of <code>train_step</code> it gets a bit
unwieldy to handle all the state. It could get worse if we later need a more
complex state. One solution would be to return a <code>namedtuple</code> so we can at
least package the state together somewhat. However, Flax provides its own
solution, <code>flax.training.train_state.TrainState</code>, which has some extra
functions that make updating the combined state (model and optimiser state)
easier.</p><p>It is easiest to show by simply taking our earlier <code>train_step</code> and refactoring
it with <code>TrainState</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> flax.training.train_state <span style=color:#f92672>import</span> TrainState
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_train_step</span>(key, model, optimiser):
</span></span><span style=display:flex><span>  params <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>init(key, jnp<span style=color:#f92672>.</span>zeros((batch_size, <span style=color:#ae81ff>784</span>)), jnp<span style=color:#f92672>.</span>zeros((batch_size, num_classes)), jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>  state <span style=color:#f92672>=</span> TrainState<span style=color:#f92672>.</span>create(apply_fn<span style=color:#f92672>=</span>model<span style=color:#f92672>.</span>apply, params<span style=color:#f92672>=</span>params, tx<span style=color:#f92672>=</span>optimiser)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>loss_fn</span>(state, x, c, key):
</span></span><span style=display:flex><span>    reduce_dims <span style=color:#f92672>=</span> list(range(<span style=color:#ae81ff>1</span>, len(x<span style=color:#f92672>.</span>shape)))
</span></span><span style=display:flex><span>    c <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>one_hot(c, num_classes)
</span></span><span style=display:flex><span>    recon, mean, logvar <span style=color:#f92672>=</span> state<span style=color:#f92672>.</span>apply_fn(state<span style=color:#f92672>.</span>params, x, c, key)
</span></span><span style=display:flex><span>    mse_loss <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>l2_loss(recon, x)<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span>reduce_dims)<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>    kl_loss <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>mean(<span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> jnp<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> logvar <span style=color:#f92672>-</span> mean <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> jnp<span style=color:#f92672>.</span>exp(logvar), axis<span style=color:#f92672>=</span>reduce_dims))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> mse_loss <span style=color:#f92672>+</span> kl_weight <span style=color:#f92672>*</span> kl_loss
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> loss, (mse_loss, kl_loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@jax.jit</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_step</span>(state, x, c, key):
</span></span><span style=display:flex><span>    losses, grads <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>value_and_grad(loss_fn, has_aux<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)(state, x, c, key)
</span></span><span style=display:flex><span>    loss, (mse_loss, kl_loss) <span style=color:#f92672>=</span> losses
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    state <span style=color:#f92672>=</span> state<span style=color:#f92672>.</span>apply_gradients(grads<span style=color:#f92672>=</span>grads)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> state, loss, mse_loss, kl_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> train_step, state
</span></span></code></pre></div><p>We begin <code>create_train_step</code> by initialising our parameters as before. However,
the next step is now to create the state using <code>TrainState.create</code> and passing
our model forward call, the initialised parameters, and the optimiser we want
to use. Internally, <code>TrainState.create</code> will initialise and store the optimiser
state for us.</p><p>In <code>loss_fn</code>, rather than call <code>model.apply</code> we can use <code>state.apply_fn</code>
instead. Either method is equivalent, just that sometimes we may not have
<code>model</code> in scope and so can&rsquo;t access <code>model.apply</code>.</p><p>The largest change is in <code>train_step</code> itself. Rather than call
<code>optimiser.update</code> followed by <code>optax.apply_updates</code>, we simply call
<code>state.apply_gradients</code> which internally updates the optimiser state and the
parameters. It then returns the new state, which we return and pass to the next
call of <code>train_step</code> – as we would with <code>params</code> and <code>opt_state</code>.</p><blockquote><p>It is possible to add extra attributes to <code>TrainState</code> by subclassing it, for
example adding attributes to store the latest loss.</p></blockquote><p>In conclusion, <code>TrainState</code> makes it easier to pass around state in the training
loop, as well as abstracting away optimiser and parameter updates.</p><p>Another useful feature of Flax is the ability to <em>bind</em> parameters to a model,
yielding an interactive instance that can be called directly, as if it were a
PyTorch model with internal state. However, this state is static and can only
change if we bind it again, which makes it unusable for training. However, it
can be handy for interactive debugging or inference.</p><p>The API is pretty simple:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>key, model_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>init(model_key, jnp<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>8</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>bound_model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>bind(params)
</span></span><span style=display:flex><span>bound_model(jnp<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>8</span>))
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: Array([ <span style=color:#ae81ff>0.45935923</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.691003</span>  ], dtype<span style=color:#f92672>=</span>float32)
</span></span></code></pre></div><p>We can get back the unbound model and its parameters by calling <code>model.unbind</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>bound_model<span style=color:#f92672>.</span>unbind()
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: (Dense(
</span></span><span style=display:flex><span>     <span style=color:#75715e># attributes</span>
</span></span><span style=display:flex><span>     features <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>     use_bias <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>     dtype <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>     param_dtype <span style=color:#f92672>=</span> float32
</span></span><span style=display:flex><span>     precision <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>     kernel_init <span style=color:#f92672>=</span> init
</span></span><span style=display:flex><span>     bias_init <span style=color:#f92672>=</span> zeros
</span></span><span style=display:flex><span>     dot_general <span style=color:#f92672>=</span> dot_general
</span></span><span style=display:flex><span> ),
</span></span><span style=display:flex><span> FrozenDict({
</span></span><span style=display:flex><span>     params: {
</span></span><span style=display:flex><span>         kernel: Array([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.11450272</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2808447</span> ],
</span></span><span style=display:flex><span>                [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.45104247</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3774913</span> ],
</span></span><span style=display:flex><span>                [ <span style=color:#ae81ff>0.07462895</span>,  <span style=color:#ae81ff>0.3622056</span> ],
</span></span><span style=display:flex><span>                [ <span style=color:#ae81ff>0.59189916</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.34050766</span>],
</span></span><span style=display:flex><span>                [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.10401642</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.36226135</span>],
</span></span><span style=display:flex><span>                [ <span style=color:#ae81ff>0.157985</span>  ,  <span style=color:#ae81ff>0.00198693</span>],
</span></span><span style=display:flex><span>                [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00792678</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1142673</span> ],
</span></span><span style=display:flex><span>                [ <span style=color:#ae81ff>0.31233454</span>,  <span style=color:#ae81ff>0.4201768</span> ]], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>         bias: Array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span>     },
</span></span><span style=display:flex><span> }))
</span></span></code></pre></div><p>I said I wouldn&rsquo;t enumerate layers in Flax as I don&rsquo;t see much value in doing
so, but I will highlight two particularly interesting ones. First is
<code>nn.Dropout</code> which is numerically the same as its PyTorch counterpart, but like
anything random in JAX, requires a PRNG key as input.</p><p>The dropout layer takes its random key by internally calling
<code>self.make_rng('dropout')</code>, which pulls and splits from a PRNG stream named
<code>'dropout'</code>. This means when we call <code>model.apply</code> we will need to define the
starting key for this PRNG stream. This can be done by passing a dictionary
mapping stream names to PRNG keys, to the <code>rngs</code> argument in <code>model.apply</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>key, x_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>key, drop_key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(x_key, (<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>3</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>, deterministic<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>apply({}, x, rngs<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;dropout&#39;</span>: drop_key}) <span style=color:#75715e># there is no state, just pass empty dictionary :)</span>
</span></span><span style=display:flex><span>x, y
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: (Array([[ <span style=color:#ae81ff>1.7353934</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.741734</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>1.3312583</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>1.615281</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>0.6381292</span>,  <span style=color:#ae81ff>1.3057163</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>1.2640097</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.986926</span> ,  <span style=color:#ae81ff>1.7818599</span>]], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span> Array([[ <span style=color:#ae81ff>3.4707868</span>,  <span style=color:#ae81ff>0.</span>       , <span style=color:#f92672>-</span><span style=color:#ae81ff>2.6625166</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>0.</span>       ,  <span style=color:#ae81ff>0.</span>       ,  <span style=color:#ae81ff>2.6114326</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>0.</span>       , <span style=color:#f92672>-</span><span style=color:#ae81ff>3.973852</span> ,  <span style=color:#ae81ff>0.</span>       ]], dtype<span style=color:#f92672>=</span>float32))
</span></span></code></pre></div><blockquote><p><code>model.init</code> also accepts a dictionary of PRNG keys. If you pass in a
single key like we have done so far, it starts a stream named <code>'params'</code>. This
is equivalent to passing <code>{'params': rng}</code> instead.</p></blockquote><p>The streams are accessible to submodules, so <code>nn.Dropout</code> can call
<code>self.make_rng('dropout')</code> regardless of where it is in the model. We can
define our own PRNG streams by specifying them in the <code>model.apply</code> call. In
our VAE example, we could forgo passing in the key manually, and instead get
keys for random sampling using <code>self.make_rng('noise')</code> or similar, then
passing a starting key in <code>rngs</code> in <code>model.apply</code>. For models with lots of
randomness, it may be worth doing this.</p><p>The second useful built-in module is <code>nn.Sequential</code> which is again like its
PyTorch counterpart. This simply chains together many modules such that the
outputs of one module will flow into the inputs of the next. Useful if we want
to define large stacks of layers quickly.</p><p>Now onto some Optax tidbits! First, Optax comes with a bunch of learning rate
schedulers. Instead of passing a float value to <code>learning_rate</code> when creating
the optimiser, we can pass a scheduler. When applying updates, Optax will
automatically select the correct learning rate. Let&rsquo;s define a simple, linear
schedule:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>start_lr, end_lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-3</span>, <span style=color:#ae81ff>1e-5</span>
</span></span><span style=display:flex><span>steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>10_000</span>
</span></span><span style=display:flex><span>lr_scheduler <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>linear_schedule(
</span></span><span style=display:flex><span>  init_value<span style=color:#f92672>=</span>start_lr,
</span></span><span style=display:flex><span>  end_value<span style=color:#f92672>=</span>end_lr,
</span></span><span style=display:flex><span>  transition_steps<span style=color:#f92672>=</span>steps,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>adam(learning_rate<span style=color:#f92672>=</span>lr_scheduler)
</span></span></code></pre></div><p>You can join together schedulers using <code>optax.join_schedules</code> in order to get
more complex behaviour like learning rate warmup followed by decay:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>warmup_start_lr, warmup_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-6</span>, <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>start_lr, end_lr, steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-2</span>, <span style=color:#ae81ff>1e-5</span>, <span style=color:#ae81ff>10_000</span>
</span></span><span style=display:flex><span>lr_scheduler <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>join_schedules(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        optax<span style=color:#f92672>.</span>linear_schedule(
</span></span><span style=display:flex><span>            warmup_start_lr,
</span></span><span style=display:flex><span>            start_lr,
</span></span><span style=display:flex><span>            warmup_steps,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        optax<span style=color:#f92672>.</span>linear_schedule(
</span></span><span style=display:flex><span>            start_lr,
</span></span><span style=display:flex><span>            end_lr,
</span></span><span style=display:flex><span>            steps <span style=color:#f92672>-</span> warmup_steps,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    [warmup_steps],
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>adam(lr_scheduler)
</span></span></code></pre></div><p>The last argument to <code>optax.join_schedules</code> should be a sequence of integers
defining the step boundaries between different schedules. In this case, we
switch from warmup to decay after <code>warmup_steps</code> steps.</p><blockquote><p>Optax keeps track of the number of optimiser steps in its <code>opt_state</code>, so we
don&rsquo;t need to track this ourselves.</p></blockquote><p>Similar to joining schedulers, Optax supports chaining optimisers together.
More specifically, the chaining of gradient transformations:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>chain(
</span></span><span style=display:flex><span>    optax<span style=color:#f92672>.</span>clip_by_global_norm(<span style=color:#ae81ff>1.0</span>),
</span></span><span style=display:flex><span>    optax<span style=color:#f92672>.</span>adam(<span style=color:#ae81ff>1e-2</span>),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>When calling <code>optimiser.update</code>, the gradients will first be clipped before
then doing the regular Adam update. Chaining together transformations like this
is quite an elegant API and allows for complex behaviour. To illustrate, adding
exponential moving averages (EMA) of our updates in something like PyTorch is
non-trivial, whereas in Optax it is as simple as adding <code>optax.ema</code> to our
<code>optax.chain</code> call:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>chain(
</span></span><span style=display:flex><span>    optax<span style=color:#f92672>.</span>clip_by_global_norm(<span style=color:#ae81ff>1.0</span>),
</span></span><span style=display:flex><span>    optax<span style=color:#f92672>.</span>adam(<span style=color:#ae81ff>1e-2</span>),
</span></span><span style=display:flex><span>    optax<span style=color:#f92672>.</span>ema(decay<span style=color:#f92672>=</span><span style=color:#ae81ff>0.999</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>In this case, <code>optax.ema</code> is a transformation on the final updates, rather than
on the unprocessed gradients.</p><p>Gradient accumulation is implemented in Optax as a optimiser wrapper, rather
than as a gradient transformation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>grad_accum <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>MultiSteps(optax<span style=color:#f92672>.</span>adam(<span style=color:#ae81ff>1e-2</span>), grad_accum)
</span></span></code></pre></div><p>The returned optimiser collects updates over the <code>optimiser.update</code> calls until
<code>grad_accum</code> steps have occurred. In the intermediate steps, the returned
updates will be a PyTree of zeros in the same shape as <code>params</code>, resulting in
no update. Every <code>grad_accum</code> steps, the accumulated updates will be returned.</p><p><code>grad_accum</code> can also be a function, which gives us a way to vary the batch
size during training via adjusting the number of steps between parameter
updates.</p><p>How about if we only want to train certain parameters? For example, when
finetuning a pretrained model. Nowadays, this is a pretty common thing to do,
taking pretrained large language models and adapting them for specific
downstream tasks.</p><p>Let&rsquo;s grab a pretrained BERT model from the Huggingface hub:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> FlaxBertForSequenceClassification
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> FlaxBertForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>params<span style=color:#f92672>.</span>keys()
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: dict_keys([<span style=color:#e6db74>&#39;bert&#39;</span>, <span style=color:#e6db74>&#39;classifier&#39;</span>])
</span></span></code></pre></div><blockquote><p>Huggingface provides Flax versions of <em>most</em> of their models. The API to use
them is a bit different, calling <code>model(**inputs, params=params)</code> rather than
<code>model.apply</code>. Providing no parameters will use the pretrained weights stored
in <code>model.params</code> which is useful for inference-only tasks, but for training we
need to pass the current parameters to the call.</p></blockquote><p>We can see there are two top-level keys in the parameter PyTree: <code>bert</code> and
<code>classifier</code>. Suppose we only want to finetune the classifier head and leave the
BERT backbone alone, we can achieve this using <code>optax.multi_transform</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimiser <span style=color:#f92672>=</span> optax<span style=color:#f92672>.</span>multi_transform({<span style=color:#e6db74>&#39;train&#39;</span>: optax<span style=color:#f92672>.</span>adam(<span style=color:#ae81ff>1e-3</span>), <span style=color:#e6db74>&#39;freeze&#39;</span>: optax<span style=color:#f92672>.</span>set_to_zero()}, {<span style=color:#e6db74>&#39;bert&#39;</span>: <span style=color:#e6db74>&#39;freeze&#39;</span>, <span style=color:#e6db74>&#39;classifier&#39;</span>: <span style=color:#e6db74>&#39;train&#39;</span>})
</span></span><span style=display:flex><span>opt_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>init(model<span style=color:#f92672>.</span>params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>grads <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>tree_map(jnp<span style=color:#f92672>.</span>ones_like, model<span style=color:#f92672>.</span>params)
</span></span><span style=display:flex><span>updates, opt_state <span style=color:#f92672>=</span> optimiser<span style=color:#f92672>.</span>update(grads, opt_state, model<span style=color:#f92672>.</span>params)
</span></span></code></pre></div><p><code>optax.multi_transform</code> takes two inputs, the first is mapping from labels to
gradient transformations. The second is a PyTree with the same structure or
prefix as the updates (in the case above we use the prefix approach) mapping to
labels. The transformation matching the label of a given update will be
applied. This allows the partitioning of parameters and applying different
updates to different parts.</p><blockquote><p>The second argument can also be a function that, given the updates PyTree,
returns such a PyTree mapping updates (or their prefix) to labels.</p></blockquote><p>This can be used for other cases like having different optimisers for different
layers (such as disabling weight decay for certain layers), but in our case we
simply use <code>optax.adam</code> for our trainable parameters, and zero out gradients
for other regions using the stateless transform <code>optax.set_to_zero</code>.</p><blockquote><p>In jit-compiled function, the gradients that have <code>optax.set_to_zero</code> applied
to them won&rsquo;t be computed due to the optimisation process seeing that they
will always be zero. Hence, we get the expected memory savings from only
finetuning a subset of layers!</p></blockquote><p>Let&rsquo;s print the updates so that we can see that we do indeed have no updates in
the BERT backbone, and have updates in the classifier head:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>updates[<span style=color:#e6db74>&#39;classifier&#39;</span>], updates[<span style=color:#e6db74>&#39;bert&#39;</span>][<span style=color:#e6db74>&#39;embeddings&#39;</span>][<span style=color:#e6db74>&#39;token_type_embeddings&#39;</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out:
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;bias&#39;</span>: Array([<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;kernel&#39;</span>: Array([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>],
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00100002</span>]], dtype<span style=color:#f92672>=</span>float32)}
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;embedding&#39;</span>: Array([[<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#f92672>...</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#f92672>...</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>]], dtype<span style=color:#f92672>=</span>float32)}
</span></span></code></pre></div><p>We can verify that all updates are zero using <code>jax.tree_util.tree_reduce</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>jax<span style=color:#f92672>.</span>tree_util<span style=color:#f92672>.</span>tree_reduce(<span style=color:#66d9ef>lambda</span> c, p: c <span style=color:#f92672>and</span> (jnp<span style=color:#f92672>.</span>count_nonzero(p) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>), updates[<span style=color:#e6db74>&#39;bert&#39;</span>], <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: Array(<span style=color:#66d9ef>True</span>, dtype<span style=color:#f92672>=</span>bool)
</span></span></code></pre></div><p>Both Flax and Optax are quite feature-rich despite the relative infancy of the
JAX ecosystem. I&rsquo;d recommend just opening the
<a href=https://flax.readthedocs.io/en/latest/api_reference/index.html>Flax</a> or
<a href=https://optax.readthedocs.io/en/latest/api.html>Optax API reference</a> and
searching for layers, optimisers, losses, and features you are used to having
in other frameworks.</p><p>The last thing I want to talk about involves an entirely different library build
on JAX. <strong>Orbax</strong> provides PyTree checkpointing utilities for saving and
restoring arbitrary PyTrees. I won&rsquo;t go into great detail but will show basic
usage here. There is nothing worse than spending hours training only to forget
about actually saving progress!</p><p>Here is basic usage saving the BERT classifier parameters:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> orbax
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> orbax.checkpoint
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> flax.training <span style=color:#f92672>import</span> orbax_utils
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>orbax_checkpointer <span style=color:#f92672>=</span> orbax<span style=color:#f92672>.</span>checkpoint<span style=color:#f92672>.</span>PyTreeCheckpointer()
</span></span><span style=display:flex><span>save_args <span style=color:#f92672>=</span> orbax_utils<span style=color:#f92672>.</span>save_args_from_target(model<span style=color:#f92672>.</span>params[<span style=color:#e6db74>&#39;classifier&#39;</span>])
</span></span><span style=display:flex><span>orbax_checkpointer<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#39;classifier.ckpt&#39;</span>, model<span style=color:#f92672>.</span>params[<span style=color:#e6db74>&#39;classifier&#39;</span>], save_args<span style=color:#f92672>=</span>save_args)
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>ls
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: classifier<span style=color:#f92672>.</span>ckpt
</span></span></code></pre></div><p>Which we can restore by executing:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>orbax_checkpointer<span style=color:#f92672>.</span>restore(<span style=color:#e6db74>&#39;classifier.ckpt&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: {<span style=color:#e6db74>&#39;bias&#39;</span>: array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;kernel&#39;</span>: array([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.06871808</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.06338844</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.03397266</span>,  <span style=color:#ae81ff>0.00899913</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00669084</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.06431466</span>],
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.02699363</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.03812294</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00148801</span>,  <span style=color:#ae81ff>0.01149782</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.01051403</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00801195</span>]], dtype<span style=color:#f92672>=</span>float32)}
</span></span></code></pre></div><p>Which returns the raw PyTree. If you are using a custom dataclass with objects
that can&rsquo;t be serialised (such as a Flax train state where <code>apply_fn</code> and <code>tx</code>
can&rsquo;t be serialised) you can pass an example PyTree to <code>item</code> in the <code>restore</code>
call, to let Orbax know the structure you want.</p><p>Manually saving checkpoints like this is a bit old-fashioned. Orbax has a bunch
of automatic versioning and scheduling features built in, such as automatic
deleting of old checkpoints, tracking the best metric, and more. To use these
features, wrap the <code>orbax_checkpointer</code> in
<code>orbax.checkpoint.CheckpointManager</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>options <span style=color:#f92672>=</span> orbax<span style=color:#f92672>.</span>checkpoint<span style=color:#f92672>.</span>CheckpointManagerOptions(max_to_keep<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, create<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>checkpoint_manager <span style=color:#f92672>=</span> orbax<span style=color:#f92672>.</span>checkpoint<span style=color:#f92672>.</span>CheckpointManager(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;managed-checkpoint&#39;</span>, orbax_checkpointer, options)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    checkpoint_manager<span style=color:#f92672>.</span>save(step, model<span style=color:#f92672>.</span>params[<span style=color:#e6db74>&#39;classifier&#39;</span>], save_kwargs<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;save_args&#39;</span>: save_args})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>!</span>ls <span style=color:#f92672>-</span>l managed<span style=color:#f92672>-</span>checkpoint<span style=color:#f92672>/*</span>
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out:
</span></span><span style=display:flex><span>managed<span style=color:#f92672>-</span>checkpoint<span style=color:#f92672>/</span><span style=color:#ae81ff>6</span>:
</span></span><span style=display:flex><span>total <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>drwxr<span style=color:#f92672>-</span>xr<span style=color:#f92672>-</span>x <span style=color:#ae81ff>2</span> root root <span style=color:#ae81ff>4096</span> Jun  <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>09</span>:<span style=color:#ae81ff>07</span> default
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>managed<span style=color:#f92672>-</span>checkpoint<span style=color:#f92672>/</span><span style=color:#ae81ff>7</span>:
</span></span><span style=display:flex><span>total <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>drwxr<span style=color:#f92672>-</span>xr<span style=color:#f92672>-</span>x <span style=color:#ae81ff>2</span> root root <span style=color:#ae81ff>4096</span> Jun  <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>09</span>:<span style=color:#ae81ff>07</span> default
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>managed<span style=color:#f92672>-</span>checkpoint<span style=color:#f92672>/</span><span style=color:#ae81ff>8</span>:
</span></span><span style=display:flex><span>total <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>drwxr<span style=color:#f92672>-</span>xr<span style=color:#f92672>-</span>x <span style=color:#ae81ff>2</span> root root <span style=color:#ae81ff>4096</span> Jun  <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>09</span>:<span style=color:#ae81ff>07</span> default
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>managed<span style=color:#f92672>-</span>checkpoint<span style=color:#f92672>/</span><span style=color:#ae81ff>9</span>:
</span></span><span style=display:flex><span>total <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>drwxr<span style=color:#f92672>-</span>xr<span style=color:#f92672>-</span>x <span style=color:#ae81ff>2</span> root root <span style=color:#ae81ff>4096</span> Jun  <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>09</span>:<span style=color:#ae81ff>07</span> default
</span></span></code></pre></div><p>As we set <code>max_to_keep=4</code>, only the last four checkpoints have been kept.</p><p>We can view which steps have checkpoints:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>checkpoint_manager<span style=color:#f92672>.</span>all_steps()
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: [<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]
</span></span></code></pre></div><p>As well as view if there is a checkpoint for a specific step:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>checkpoint_manager<span style=color:#f92672>.</span>should_save(<span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: <span style=color:#66d9ef>False</span>
</span></span></code></pre></div><p>And what the latest saved step was:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>checkpoint_manager<span style=color:#f92672>.</span>latest_step()
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: <span style=color:#ae81ff>9</span>
</span></span></code></pre></div><p>We can restore using the checkpoint manager. Rather than provide a path to the
<code>restore</code> function, we provide the step we want to restore:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>step <span style=color:#f92672>=</span> checkpoint_manager<span style=color:#f92672>.</span>latest_step()
</span></span><span style=display:flex><span>checkpoint_manager<span style=color:#f92672>.</span>restore(step)
</span></span><span style=display:flex><span><span style=color:#f92672>===</span>
</span></span><span style=display:flex><span>Out: {<span style=color:#e6db74>&#39;bias&#39;</span>: array([<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>], dtype<span style=color:#f92672>=</span>float32),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;kernel&#39;</span>: array([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.06871808</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.06338844</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.03397266</span>,  <span style=color:#ae81ff>0.00899913</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00669084</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.06431466</span>],
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.02699363</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.03812294</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.00148801</span>,  <span style=color:#ae81ff>0.01149782</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.01051403</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.00801195</span>]], dtype<span style=color:#f92672>=</span>float32)}
</span></span></code></pre></div><p>For especially large checkpoints, Orbax supports asynchronous checkpointing
which moves checkpointing to a background thread. You can do this by wrapping
<code>orbax.checkpoint.AsyncCheckpointer</code> around the
<code>orbax.checkpoint.PyTreeCheckpointer</code> we created earlier.</p><blockquote><p>You may see mention online to Flax checkpointing utilities. However, these
utilities are being deprecated and it is recommended to start using Orbax
instead.</p></blockquote><p>The documentation for Orbax is a bit spartan, but it has a fair few options to
choose. It is worth just reading the <code>CheckpointManagerOptions</code> class
<a href=https://github.com/google/orbax/blob/main/checkpoint/orbax/checkpoint/checkpoint_manager.py#L85>here</a>
and seeing the available features.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In this blog post, I&rsquo;ve introduced two libraries built on top of JAX: Flax and
Optax. This has been more of a practical guide into how you can implement
training loops easily in JAX using these libraries, rather than a ideological
discussion like my previous blog post on JAX.</p><p>To summarise this post:</p><ul><li>Flax provides a neural network API that allows the developer to build neural
network modules in a class-based way. Unlike other frameworks, these modules
do not contain state within them, essentially hollow shells that loosely
associate functions with parameters and inputs, and provide easy methods to
initialise the parameters.</li><li>Optax provides a large suite of optimisers for updating our parameters.
These, like Flax modules, do not contain state and must have state passed
manually to it. All optimisers are simply gradient transformations: a
pair of pure functions <code>init</code> and <code>update</code>. Optax also provides other
gradient transformations and wrappers to allow for more complex behaviour,
such as gradient clipping and parameter freezing.</li><li>Both libraries simply operate on and return PyTrees and can easily
interoperate with base JAX - crucially with <code>jax.jit</code>. This also makes them
interoperable with other libraries based on JAX. For example, by choosing
Flax, we aren&rsquo;t locked into using Optax, and vice versa.</li></ul><p>There is a lot more to these two libraries than described here, but I hope this
is a good starting point and can enable you to create your own training loops
in JAX. A good exercise now would be to use the training loop and model code in
this blog post and adapting it for your own tasks, such as another generative
model.</p><p>If you liked this post please consider following me on
<a href=https://twitter.com/alexfmckinney>Twitter</a> or use this site&rsquo;s RSS feed for
notifications on future ramblings about machine learning and other topics.
Alternatively you can navigate to the root of this website and repeatedly
refresh until something happens. Thank you for reading this far and I hope you
found it useful!</p><hr><h3 id=acknowledgements-and-extra-resources>Acknowledgements and Extra Resources<a hidden class=anchor aria-hidden=true href=#acknowledgements-and-extra-resources>#</a></h3><p>Some good extra resources:</p><ul><li><a href=https://afmck.in/posts/2023-05-22-jax-post/>My previous blog post on JAX</a></li><li><a href=https://github.com/gordicaleksa/get-started-with-JAX>Aleksa Gordic’s JAX and Flax tutorial series</a></li><li><a href=https://flax.readthedocs.io/en/latest/>Flax documentation</a></li><li><a href=https://optax.readthedocs.io/en/latest/>Optax documentation</a></li><li><a href=https://github.com/patrick-kidger/equinox>Equinox - another neural network library built on JAX by Patrick Kidger</a></li><li><a href=https://github.com/deepmind/dm-haiku>Haiku - another neural network library built on JAX from Deepmind</a></li><li><a href=https://github.com/google/orbax>Orbax source code</a></li></ul><p><em>Found something wrong with this blog post? Let me know via email or Twitter!</em></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://afmck.in>Alex McKinney</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
<a href=/cheems rel="noopener noreferrer">🐕️</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>